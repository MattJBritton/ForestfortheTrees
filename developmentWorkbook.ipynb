{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RendererRegistry.enable('notebook')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#imports\n",
    "from __future__ import division\n",
    "import time\n",
    "import datetime\n",
    "import copy\n",
    "from itertools import product\n",
    "import operator\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import RandomForestClassifier,\\\n",
    "    GradientBoostingRegressor,\\\n",
    "    RandomForestRegressor,\\\n",
    "    GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "\n",
    "import altair as alt\n",
    "alt.renderers.enable(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForestForTheTrees:\n",
    "    \n",
    "    DEFAULT_SAMPLE_SIZE = 500\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.dataset = None\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        self.feature_names = None\n",
    "        self.feature_locs = None\n",
    "        self.target_type = None\n",
    "        self.classifier_type = None\n",
    "        self.classifier = None\n",
    "        self.feature_ranges = {}\n",
    "        self.mean_prediction = None\n",
    "        self.offset = None\n",
    "        self.no_predictor_features = []\n",
    "        self.oned_features = []   \n",
    "        self.binned_data = None\n",
    "        self.sample_size = self.DEFAULT_SAMPLE_SIZE\n",
    "        \n",
    "    def set_sample_size(self, new_size):\n",
    "        self.sample_size = new_size\n",
    "    \n",
    "    def get_dataset(self, dataset):\n",
    "        \n",
    "        if dataset == \"breast cancer\":\n",
    "            dataLoad = datasets.load_breast_cancer(return_X_y=False)\n",
    "            return {\n",
    "                \"x\": dataBunch.data[:,:10],\n",
    "                \"y\": dataBunch.target,\n",
    "                \"feature_names\": dataBunch.feature_names[:10],\n",
    "                \"feature_locs\": {x:i for i,x in enumerate(dataBunch.feature_names[:10])},\n",
    "                \"target_type\": \"Classification\"\n",
    "            }     \n",
    "        elif dataset == \"cervical cancer\":\n",
    "            dataLoad = pd.read_csv(\"data/cervical_cancer.csv\")\n",
    "            target = dataLoad.Biopsy\n",
    "            dataLoad = dataLoad.drop([\"Person\", \"Biopsy\"],axis=1)\n",
    "            return {\n",
    "                \"x\": dataLoad.values,\n",
    "                \"y\": target,\n",
    "                \"feature_names\": dataLoad.columns,\n",
    "                \"feature_locs\": {x:i for i,x in enumerate(dataLoad.columns)},\n",
    "                \"target_type\": \"Classification\"\n",
    "            }\n",
    "        elif dataset == \"bike\":\n",
    "            def _datestr_to_timestamp(s):\n",
    "                return time.mktime(datetime.datetime.strptime(s, \"%Y-%m-%d\").timetuple())\n",
    "\n",
    "            dataLoad = pd.read_csv('data/bike.csv')\n",
    "            dataLoad['dteday'] = dataLoad['dteday'].apply(_datestr_to_timestamp)\n",
    "            dataLoad = pd.get_dummies(dataLoad, prefix=[\"weathersit\"], columns=[\"weathersit\"], drop_first=False)\n",
    "\n",
    "            #de-normalize data to produce human-readable features.\n",
    "            #Original range info from http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\n",
    "            dataLoad[\"hum\"] = dataLoad[\"hum\"].apply(lambda x: x*100.)\n",
    "            dataLoad[\"windspeed\"] = dataLoad[\"windspeed\"].apply(lambda x: x*67.)\n",
    "            #convert Celsius to Fahrenheit\n",
    "            dataLoad[\"temp\"] = dataLoad[\"temp\"].apply(lambda x: (x*47. - 8)*9/5 +32)\n",
    "            dataLoad[\"atemp\"] = dataLoad[\"atemp\"].apply(lambda x: (x*66. - 16)*9/5 + 32)\n",
    "\n",
    "            #rename features to make them interpretable for novice users\n",
    "            feature_names_dict = {\n",
    "                \"yr\":\"First or Second Year\", \n",
    "                \"season\":\"Season\", \n",
    "                \"hr\":\"Hour of Day\", \n",
    "                \"workingday\":\"Work Day\",\n",
    "                \"weathersit_2\":\"Misty Weather\",\n",
    "                \"weathersit_3\":\"Light Precipitation\",\n",
    "                \"weathersit_4\":\"Heavy Precipitation\",\n",
    "                \"temp\":\"Temperature (F)\",\n",
    "                \"atemp\":\"Feels Like (F)\",\n",
    "                \"hum\":\"Humidity\",\n",
    "                \"windspeed\":\"Wind Speed\"\n",
    "            }\n",
    "            dataLoad = dataLoad.rename(mapper=feature_names_dict,axis=1) \n",
    "            features = feature_names_dict.values()\n",
    "\n",
    "            return {\n",
    "                \"x\": dataLoad[features].values,\n",
    "                \"y\": dataLoad[\"cnt\"],\n",
    "                \"feature_names\": features,\n",
    "                \"feature_locs\": {x:i for i,x in enumerate(features)},\n",
    "                \"target_type\": \"Regression\"\n",
    "            }\n",
    "\n",
    "    def bin_data(self):\n",
    "    \n",
    "        prediction_contributions = pd.DataFrame(\n",
    "            self.get_sample(self.x),\n",
    "            columns = self.feature_names\n",
    "        )\n",
    "        for key in self.get_feature_pairs():\n",
    "            tempH = np.digitize(\n",
    "                prediction_contributions.loc[:,key[0]],\n",
    "                self.feature_ranges[key[0]]\n",
    "            )-1.\n",
    "            tempV = np.digitize(\n",
    "                prediction_contributions.loc[:,key[1]],\n",
    "                self.feature_ranges[key[1]]\n",
    "            )-1.\n",
    "            prediction_contributions[key] = prediction_contributions.apply(lambda x:\\\n",
    "                                                    int(tempV*len(self.feature_ranges[key[0]]) + tempH),\\\n",
    "                                                    axis=1)\n",
    "        return prediction_contributions        \n",
    "        \n",
    "    def load_dataset(self, dataset):\n",
    "\n",
    "        self.dataset = dataset\n",
    "        data = self.get_dataset(self.dataset)\n",
    "        self.x = data[\"x\"]\n",
    "        self.y = data[\"y\"]\n",
    "        self.feature_names = data[\"feature_names\"]\n",
    "        self.feature_locs = data[\"feature_locs\"]\n",
    "        self.target_type = data[\"target_type\"]\n",
    "        self.feature_ranges = {\n",
    "            feature : self.get_quantiles(feature)\n",
    "            for feature in self.feature_names\n",
    "        }  \n",
    "        self.binned_data = self.bin_data()    \n",
    "            \n",
    "    def build_base_model(self, num_estimators, model_type):\n",
    "\n",
    "        model_lookup_dict = {\n",
    "            (\"classification\", \"random forest\") : RandomForestClassifier,\n",
    "            (\"classification\", \"gradient boosting\") : GradientBoostingClassifier,\n",
    "            (\"regression\", \"random forest\") : RandomForestRegressor,\n",
    "            (\"regression\", \"gradient boosting\") : GradientBoostingRegressor\n",
    "        }\n",
    "\n",
    "        self.model_type = model_type\n",
    "        self.classifier_type = model_lookup_dict[(self.target_type, self.model_type)]\n",
    "\n",
    "        self.model = self.classifier_type(n_estimators=num_estimators, max_depth=2)\n",
    "        self.model.fit(self.x, self.y)\n",
    "        self.pred_y = model.predict(self.x)\n",
    "\n",
    "    def get_model_accuracy(self):\n",
    "        if self.target_type == \"classification\":\n",
    "            return accuracy_score(self.y, self.pred_y)\n",
    "        else:\n",
    "            return r2_score(self.y, self.pred_y)    \n",
    "        \n",
    "    def _get_coordinate_matrix(lst, length, direction):\n",
    "        if direction==\"h\":\n",
    "            return lst*length\n",
    "        else:\n",
    "            return [item for item in lst\\\n",
    "             for i in range(length)]   \n",
    "\n",
    "    def get_quantile_matrix(self, feat1, feat2):\n",
    "        h = _get_coordinate_matrix(\n",
    "            list(self.feature_ranges[feat1]),\n",
    "            len(self.feature_ranges[feat2]),\n",
    "            \"h\"\n",
    "        )\n",
    "        v = _get_coordinate_matrix(\n",
    "            list(self.feature_ranges[feat2]),\n",
    "            len(self.feature_ranges[feat1]),\n",
    "            \"v\"\n",
    "        )                      \n",
    "        return h,v \n",
    "\n",
    "    def get_leaf_value(self, node_position):\n",
    "        node = self.model.tree_.value[node_position]\n",
    "        if self.target_type == \"Classification\":\n",
    "            return node[0][1]/(node[0][1] + node[0][0])\n",
    "        else:\n",
    "            return node        \n",
    "\n",
    "    def get_feature_pair_key(self, feat1, feat2):\n",
    "        if self.feature_ranges[feat1].shape[0] == self.feature_ranges[feat2].shape[0]:\n",
    "            #need stable order so keys with same number of quantiles appear in only one order\n",
    "            return tuple(sorted([feat1, feat2]))\n",
    "        elif self.feature_ranges[feat1].shape[0] > self.feature_ranges[feat2].shape[0]:\n",
    "            return tuple([feat1, feat2])\n",
    "        else:\n",
    "            return tuple([feat2, feat1])        \n",
    "\n",
    "    def get_quantiles(self, feat):\n",
    "        loc = self.feature_locs[feat]\n",
    "        if np.unique(self.x[:,loc]).shape[0] < 30 or type(self.x[0,loc]) is str: #is categorical/ordinal?\n",
    "            return np.unique(self.x[:,loc])\n",
    "        else:\n",
    "            if quantiles:\n",
    "                return np.around(\n",
    "                    np.unique(\n",
    "                        np.quantile(\n",
    "                            a=self.x[:,loc],\n",
    "                            q=np.linspace(0, 1, num_tiles)\n",
    "                        )\n",
    "                    ),\n",
    "                    1)\n",
    "            else:\n",
    "                return np.around(\n",
    "                    np.linspace(\n",
    "                        np.min(self.x[:,loc]), \n",
    "                        np.max(self.x[:,loc]),\n",
    "                        num_tiles\n",
    "                    )\n",
    "                    ,1)  \n",
    "            \n",
    "    def reduce_to_1d(self, arr, threshold, direction):\n",
    "        if direction == \"h\":\n",
    "            reduced_arr = arr - arr[:,0].reshape(-1,1)\n",
    "        else:\n",
    "            reduced_arr = arr - arr[0,:].reshape(1,-1)\n",
    "        return (np.max(np.abs(reduced_arr))/np.max(np.abs(arr))) <= threshold               \n",
    "        \n",
    "    def get_sample(self, arr):\n",
    "        return arr[:self.sample_size,:]\n",
    "    \n",
    "    def get_predictions_base(self):\n",
    "        return np.full((sample_size,1), self.offset + np.mean(self.y))\n",
    "    \n",
    "    def get_explanation_accuracy(self, explanation_predictions):\n",
    "        if self.target_type == \"Regression\":\n",
    "            return r2_score(get_sample(self.pred_y), explanation_predictions)\n",
    "        \n",
    "    def get_prediction_contributions(self, chart, data_positions):\n",
    "        return np.take(chart, data_positions)   \n",
    "    \n",
    "    def sum_arrays(self, keyMain, keyAdd, arr_to_add):\n",
    "        return temp_outputs[keyMain][\"output\"]\\\n",
    "    + temp_outputs[keyAdd][arr_to_add].reshape(\n",
    "            temp_outputs[keyMain][\"output\"].shape[0]\n",
    "            if(keyMain[1]==keyAdd[1] or keyMain[1]==keyAdd[0])\n",
    "            else 1,-1\n",
    "        )\n",
    "\n",
    "    def evaluate_single_explanation(self, components, explanation):\n",
    "        \n",
    "        return get_explanation_accuracy(\n",
    "            get_predictions_base() +\\\n",
    "            np.sum(\n",
    "                np.array([\n",
    "                    get_prediction_contributions(\n",
    "                        components[expKey][\"output\"],\n",
    "                        self.binned_data[expKey]\n",
    "                    ) for expKey in explanation\n",
    "                ]), \n",
    "                axis=0\n",
    "            ).reshape(-1,1)\n",
    "        )\n",
    "\n",
    "    def copy_chart_components(self):\n",
    "        return copy.deepcopy(self.chart_components)  \n",
    "    \n",
    "    def get_feature_pairs(self):\n",
    "        return [\n",
    "            self.get_feature_pair_key(key[0], key[1])\n",
    "            for key in [tuple(t) for t in product(self.feature_names, repeat = 2)]\n",
    "        ]       \n",
    "\n",
    "    def rollup_components(self, explanation):\n",
    "        temp_outputs = self.copy_chart_components()\n",
    "        for keyRollup in [k for k in self.chart_components.iterkeys() if k not in explanation]:\n",
    "            #print \"rollup: \" + str(keyRollup)\n",
    "            hUsed = False\n",
    "            vUsed = False\n",
    "            for keyExisting in explanation:\n",
    "                #print \"try against: \" + str(keyExisting)\n",
    "                if (keyRollup[1] == keyExisting[0] or keyRollup[1] == keyExisting[1]) and not hUsed:\n",
    "                    hUsed = True\n",
    "                    if vUsed:\n",
    "                        #print \"HReduce\"\n",
    "                        temp_outputs[keyExisting][\"output\"] = sum_arrays(\n",
    "                            keyExisting, \n",
    "                            keyRollup,\n",
    "                            \"output_HReduced\"\n",
    "                        )\n",
    "                        break\n",
    "                    else:\n",
    "                        #print \"HAll\"\n",
    "                        temp_outputs[keyExisting][\"output\"] = sum_arrays(\n",
    "                            keyExisting, \n",
    "                            keyRollup,\n",
    "                            \"output_H\"\n",
    "                        )                           \n",
    "                elif (keyRollup[0] == keyExisting[0] or keyRollup[0] == keyExisting[1]) and not vUsed:\n",
    "                    vUsed = True\n",
    "                    if hUsed:\n",
    "                        #print \"VReduce\"\n",
    "                        temp_outputs[keyExisting][\"output\"] = sum_arrays(\n",
    "                            keyExisting, \n",
    "                            keyRollup,\n",
    "                            \"output_VReduced\"\n",
    "                        )                          \n",
    "                        break\n",
    "                    else:\n",
    "                        #print \"VAll\"\n",
    "                        temp_outputs[keyExisting][\"output\"] = sum_arrays(\n",
    "                            keyExisting, \n",
    "                            keyRollup,\n",
    "                            \"output_V\"\n",
    "                        )  \n",
    "        return temp_outputs    \n",
    "    \n",
    "    def extract_components(self, num_tiles = 20, collapse_1d = True, quantiles = False):\n",
    "\n",
    "        #generate data structure for pairwise charts\n",
    "        feature_pairs = {\n",
    "            key : {\n",
    "                \"map\":None,\n",
    "                \"predicates\":[]\n",
    "            }\n",
    "            for key in self.get_feature_pairs()\n",
    "        }      \n",
    "\n",
    "        for key, value in feature_pairs.iteritems():\n",
    "            h, v = self.get_quantile_matrix(key[0], key[1])\n",
    "            value[\"map\"] = np.array(\n",
    "                [\n",
    "                    {\n",
    "                        key[0]:x,\n",
    "                        key[1]:y\n",
    "                    }\n",
    "                    for x,y in zip(h,v)\n",
    "                ]\n",
    "            ).reshape(len(self.feature_ranges[key[1]]), len(self.feature_ranges[key[0]]))\n",
    "\n",
    "        for modelT in self.model.estimators_:\n",
    "            if self.target_type == \"regression\":\n",
    "                model = modelT[0]\n",
    "            else:\n",
    "                model = modelT\n",
    "            feature_ids = {i : \n",
    "                           {\n",
    "                               \"number\":x,\n",
    "                               \"name\":self.feature_names[x]\n",
    "                           } for i,x in enumerate(list(self.model.tree_.feature)) if x >= 0} #-2 means leaf node\n",
    "\n",
    "            #for 1-layer trees\n",
    "            if self.model.tree_.feature[1] <0:\n",
    "                print \"1-layer tree\"\n",
    "                print key\n",
    "                feature_pair_key = self.get_feature_pair_key(\n",
    "                    feature_ids[0][\"name\"],\n",
    "                    feature_ids[0][\"name\"]\n",
    "                )\n",
    "                decision_func_dict = {\n",
    "                    \"feature_name\": feature_ids[0][\"name\"],\n",
    "                    \"threshold\": self.model.tree_.threshold[0],\n",
    "                    \"operator\": operator.le,\n",
    "                    \"prob_le\": get_leaf_value(1),\n",
    "                    \"prob_gt\": get_leaf_value(2)\n",
    "                }       \n",
    "                #build the predictive function used in the decision tree\n",
    "                def dt_predicate(data_case, decision_func_dict=decision_func_dict):\n",
    "                    if decision_func_dict[\"operator\"](\\\n",
    "                                                        data_case[decision_func_dict[\"feature_name\"]],\\\n",
    "                                                        decision_func_dict[\"threshold\"]\\\n",
    "                                                       ):\n",
    "                        return decision_func_dict[\"prob_le\"]\n",
    "                    else:\n",
    "                        return decision_func_dict[\"prob_gt\"]        \n",
    "            else:\n",
    "                for node_position in [1,4]: #positions for left and right nodes at layer 2\n",
    "                    if node_position in feature_ids:\n",
    "                        feature_pair_key = get_feature_pair_key(\n",
    "                            feature_ids[0][\"name\"], \n",
    "                            feature_ids[node_position][\"name\"]\n",
    "                        )\n",
    "                        #get the decision rules\n",
    "                        decision_func_dict = {\n",
    "                            \"feature_name_1\": feature_ids[0][\"name\"],\n",
    "                            \"threshold_1\": self.model.tree_.threshold[0],\n",
    "                            \"operator_1\": operator.le if node_position == 1 else operator.gt,\n",
    "\n",
    "                            \"feature_name_2\": feature_ids[node_position][\"name\"],\n",
    "                            \"threshold_2\": self.model.tree_.threshold[node_position],\n",
    "                            \"operator_2\": operator.le,\n",
    "\n",
    "                            \"prob_le\": get_leaf_value(node_position+1),\n",
    "                            \"prob_gt\": get_leaf_value(node_position+2)\n",
    "                        }\n",
    "                        #print decision_func_dict\n",
    "                        #build the predictive function used in the decision tree\n",
    "                        def dt_predicate(data_case, decision_func_dict=decision_func_dict):\n",
    "                            #print data_case\n",
    "                            #print decision_func_dict\n",
    "                            if decision_func_dict[\"operator_1\"](\\\n",
    "                                                                data_case[decision_func_dict[\"feature_name_1\"]],\\\n",
    "                                                                decision_func_dict[\"threshold_1\"]\\\n",
    "                                                               ):\n",
    "                                #print \"in1\"\n",
    "                                if decision_func_dict[\"operator_2\"](\\\n",
    "                                                                    data_case[decision_func_dict[\"feature_name_2\"]],\\\n",
    "                                                                    decision_func_dict[\"threshold_2\"]\\\n",
    "                                                                   ):\n",
    "                                    #print \"in2\"\n",
    "                                    return decision_func_dict[\"prob_le\"]\n",
    "                                else:\n",
    "                                    #print \"in3\"\n",
    "                                    return decision_func_dict[\"prob_gt\"]\n",
    "                            else:\n",
    "                                return 0.\n",
    "\n",
    "                    else: #asymmetric tree, this is a leaf node\n",
    "                        feature_pair_key = get_feature_pair_key(feature_ids[0][\"name\"], feature_ids[0][\"name\"])\n",
    "                        decision_func_dict = {\n",
    "                            \"feature_name\": feature_ids[0][\"name\"],\n",
    "                            \"threshold\": model.tree_.threshold[0],\n",
    "                            \"operator\": operator.le if node_position == 1 else operator.gt,\n",
    "                            \"prob\": model.tree_.value[node_position]\n",
    "                        }\n",
    "                        #build the predictive function used in the decision tree\n",
    "                        def dt_predicate(data_case, decision_func_dict=decision_func_dict):\n",
    "                            #print data_case\n",
    "                            if decision_func_dict[\"operator\"](\\\n",
    "                                                                data_case[decision_func_dict[\"feature_name\"]],\\\n",
    "                                                                decision_func_dict[\"threshold\"]\\\n",
    "                                                               ):\n",
    "                                return decision_func_dict[\"prob\"]\n",
    "                            else:                         \n",
    "                                return 0.                 \n",
    "\n",
    "                    feature_pairs[feature_pair_key][\"predicates\"].append(dt_predicate)\n",
    "\n",
    "        #now calculate output array for each feature pair\n",
    "        for key, value in feature_pairs.iteritems():\n",
    "            arrs = []\n",
    "            for predicate in value[\"predicates\"]:\n",
    "                f = np.vectorize(predicate)\n",
    "                arrs.append(f(value[\"map\"]))\n",
    "            if len(arrs) > 0:\n",
    "                #details of vote aggreggation method for random forest\n",
    "                #https://stats.stackexchange.com/questions/127077/random-forest-probabilistic-prediction-vs-majority-vote\n",
    "                value[\"output\"] = np.sum(np.stack(arrs, axis=-1), axis=-1)\n",
    "            else:\n",
    "                value[\"output\"] = None\n",
    "\n",
    "        #build chart data\n",
    "        for key, value in feature_pairs.iteritems():\n",
    "            h,v = get_quantile_matrix(key[0], key[1])\n",
    "            value[\"H_Indices\"] = h\n",
    "            value[\"V_Indices\"] = v    \n",
    "\n",
    "        self.offset = 0\n",
    "        self.no_predictor_features = []\n",
    "        self.oned_features = []\n",
    "        chart_data = {}\n",
    "        for key, value in feature_pairs.iteritems(): \n",
    "            newKey = key\n",
    "            if value[\"output\"] is None:\n",
    "                no_predictor_features.append(key)\n",
    "                value[\"removed\"] = True\n",
    "            else:          \n",
    "                if collapse_1d:\n",
    "                    if reduce_to_1d(value[\"output\"], 0., \"v\"):\n",
    "                        newKey = key[1]\n",
    "                        value[\"output\"] = value[\"output\"][0,:]\n",
    "                        value[\"H_Indices\"] = self.feature_ranges[newKey]\n",
    "                        value[\"V_Indices\"] = None\n",
    "                        value[\"1d_key\"] = newKey\n",
    "                        value[\"removed\"] = True\n",
    "                        oned_features.append(key)                 \n",
    "                    elif reduce_to_1d(value[\"output\"], 0., \"h\"):\n",
    "                        newKey = key[0]\n",
    "                        value[\"output\"] = value[\"output\"][:,0]\n",
    "                        value[\"H_Indices\"] = feature_ranges[newKey]\n",
    "                        value[\"V_Indices\"] = None\n",
    "                        value[\"1d_key\"] = newKey\n",
    "                        value[\"removed\"] = True\n",
    "                        oned_features.append(key)\n",
    "\n",
    "                #subtract mean from votes to center at zero\n",
    "                vote_mean = value[\"output\"].mean()\n",
    "                self.offset += vote_mean\n",
    "                value[\"output\"] = value[\"output\"] - vote_mean\n",
    "\n",
    "\n",
    "        #do another loop through chart_data to push 1d charts into 2d\n",
    "        if collapse_1d:\n",
    "            for value in feature_pairs.itervalues():\n",
    "                if value[\"V_Indices\"] is None:\n",
    "                    key = value[\"1d_key\"]\n",
    "                    #get list of charts with this feature\n",
    "                    matchList = sorted([{\"key\": kInner, \"feature_importance\": np.std(vInner[\"output\"])}\\\n",
    "                                        for kInner, vInner in feature_pairs.iteritems()\\\n",
    "                                        if \"removed\" not in vInner and key in kInner],\\\n",
    "                                       key=lambda x: x[\"feature_importance\"], reverse=True)\n",
    "\n",
    "                    if len(matchList) > 0:\n",
    "                        matchKey = matchList[0][\"key\"]\n",
    "                        feature_pairs[matchKey][\"output\"] = feature_pairs[matchKey][\"output\"]\\\n",
    "                        + value[\"output\"].reshape(\\\n",
    "                                                  -1 if key==matchKey[1] else 1,\\\n",
    "                                                  -1 if key==matchKey[0] else 1\\\n",
    "                                                 )\n",
    "\n",
    "        #one last loop to generate the horizontal and vertical components\n",
    "        for key, value in feature_pairs.iteritems():\n",
    "            if \"removed\" in value:\n",
    "                pass\n",
    "            else:\n",
    "                value[\"output_H\"] = np.mean(value[\"output\"], axis=1).reshape(-1,1)\n",
    "                value[\"output_V\"] = np.mean(value[\"output\"], axis=0).reshape(1,-1)\n",
    "                value[\"output_HReduced\"] = np.mean(value[\"output\"] - value[\"output_V\"].reshape(1,-1), axis=1)\\\n",
    "                .reshape(1,-1)\n",
    "                value[\"output_VReduced\"] = np.mean(value[\"output\"] - value[\"output_H\"].reshape(-1,1), axis=0)\\\n",
    "                .reshape(-1,1)\n",
    "\n",
    "        #remove deleted keys\n",
    "        feature_pairs = {key:val for key, val in feature_pairs.iteritems() if \"removed\" not in val}\n",
    "        feature_pairs = OrderedDict(sorted(feature_pairs.items(),\\\n",
    "                                            key=lambda x: np.std(x[1][\"output\"]), reverse=True))\n",
    "        self.chart_components = {key: {\n",
    "            \"output\" : val[\"output\"],\n",
    "            \"output_VReduced\" : val[\"output_VReduced\"],\n",
    "            \"output_H\" : val[\"output_H\"],\n",
    "            \"output_HReduced\" : val[\"output_HReduced\"],\n",
    "            \"output_V\" : val[\"output_V\"]\n",
    "        } for key, val in feature_pairs.iteritems()}\n",
    "\n",
    "        self.chart_indices = {key: {\n",
    "            \"H_Indices\" : val[\"H_Indices\"],\n",
    "            \"V_Indices\" : val[\"V_Indices\"]\n",
    "        } for key, val in feature_pairs.iteritems()}  \n",
    "        \n",
    "    def explain(self, sample_size, fidelity_threshold = 1., rollup = None):\n",
    "\n",
    "        explanation = []\n",
    "        evaluation_details = []        \n",
    "        evaluation_details.append(\n",
    "            {\n",
    "                \"score\": self.get_explanation_accuracy(\n",
    "                    self.chart_components, \n",
    "                    base_predictions\n",
    "                )\n",
    "            }\n",
    "        )  \n",
    "\n",
    "        while evaluation_details[-1][\"score\"] < fidelity_threshold and len(explanation) < len(chart_components):\n",
    "            current_details = {}\n",
    "            keys_to_evaluate = [key for key in chart_components.iterkeys() if key not in explanation]\n",
    "            for key in keys_to_evaluate:\n",
    "                #print \"key to evaluate: \" + str(key)\n",
    "                #roll up other keys\n",
    "                current_explanation = explanation+[key]\n",
    "                temp_outputs = self.rollup_components(current_explanation)\\\n",
    "                if rollup == \"advanced\"\\\n",
    "                else self.copy_chart_components()\n",
    "\n",
    "                current_details[key] = self.evaluate_single_explanation(temp_outputs, current_explanation)\n",
    "\n",
    "            #get key with highest fidelity score\n",
    "            best_key = max(\n",
    "                current_details.iterkeys(),\\\n",
    "                key = (lambda key: current_details[key])\n",
    "            )\n",
    "            explanation.append(best_key)\n",
    "            current_details[\"best_key\"] = best_key\n",
    "\n",
    "            if rollup == \"simple\":\n",
    "                temp_outputs = self.rollup_components(explanation)\n",
    "                current_details[best_key] = self.evaluate_single_explanation(temp_outputs, explanation)\n",
    "\n",
    "            current_details[\"score\"] = current_details[best_key]\n",
    "            evaluation_details.append(current_details)\n",
    "            print  \"*******\"\n",
    "            print len(evaluation_details)\n",
    "            print best_key\n",
    "            print current_details[\"score\"]\n",
    "\n",
    "        self.explanation_components = temp_outputs   \n",
    "        \n",
    "    def visualize(self, plot_points = True):\n",
    "        self.scheme = \"redyellowblue\"\n",
    "        i = 1\n",
    "        rows = []\n",
    "        charts = []\n",
    "        for key, value in self.chart_indices.iteritems():\n",
    "\n",
    "            chart_df = pd.DataFrame(\n",
    "                np.hstack(\n",
    "                    value[\"H_Indices\"],\n",
    "                    value[\"V_Indices\"],\n",
    "                    self.explanation_components[key]\n",
    "                ),\n",
    "                columns = [\"H_Indices\", \"V_Indices\", \"Votes\"]\n",
    "            )\n",
    "\n",
    "            if value[\"dimension\"] == 2:\n",
    "                y_encoding = alt.Y(field = \"V_Indices\",\n",
    "                                   type = \"ordinal\", sort = \"descending\",\n",
    "                                   axis = alt.Axis(title = key[1]))                \n",
    "\n",
    "            x_encoding = alt.X(field = \"H_Indices\",\n",
    "                               type = \"ordinal\", sort = \"ascending\",\n",
    "                               axis = alt.Axis(title = key[0] if value[\"dimension\"] == 2 else key))\n",
    "\n",
    "            color_encoding = alt.Color(field = \"Votes\",\n",
    "                                       type = \"quantitative\",\n",
    "                                       scale = alt.Scale(scheme = self.scheme),\n",
    "                                       legend = alt.Legend(title = \"Votes\"))\n",
    "\n",
    "            chart = alt.Chart(data = chart_df).mark_rect()\n",
    "\n",
    "            if value[\"dimension\"] == 1:\n",
    "                chart = chart.encode(x = x_encoding, color = color_encoding)\\\n",
    "                    .properties(width = 150, height = 20)\n",
    "            else:\n",
    "                 chart = chart.encode(x = x_encoding, y = y_encoding, color = color_encoding)\\\n",
    "                    .properties(width = 150, height = 150)\n",
    "\n",
    "            if plot_points:\n",
    "                df = pd.DataFrame(self.x[np.random.choice(self.x.shape[0],300,replace=False),:],\\\n",
    "                                  columns = self.feature_names)\n",
    "                points = alt.Chart(df).mark_circle(\n",
    "                    color='black',\n",
    "                    size=5\n",
    "                ).encode(\n",
    "                    x=alt.X(field=key[0],type=\"quantitative\", sort=\"ascending\", axis=None),\n",
    "                    y=alt.X(field=key[1],type=\"quantitative\", sort=\"ascending\", axis=None)\n",
    "                ).properties(width=150, height=150)\n",
    "                chart = chart + points\n",
    "\n",
    "            charts.append(chart)\n",
    "            if len(charts) == 4 or i == len(data):\n",
    "                rows.append(alt.hconcat(*charts))\n",
    "                charts = []\n",
    "            i += 1\n",
    "        return alt.vconcat(*rows)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'quantiles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-2a5a379cb475>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mf3t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mForestForTheTrees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mf3t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bike\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mf3t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_base_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gradient boosting\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mf3t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_components\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mf3t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"simple\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-9790c16d47d1>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    116\u001b[0m         self.feature_ranges = {\n\u001b[1;32m    117\u001b[0m             \u001b[0mfeature\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_quantiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         }  \n\u001b[1;32m    120\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinned_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbin_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-9790c16d47d1>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m((feature,))\u001b[0m\n\u001b[1;32m    116\u001b[0m         self.feature_ranges = {\n\u001b[1;32m    117\u001b[0m             \u001b[0mfeature\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_quantiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         }  \n\u001b[1;32m    120\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinned_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbin_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-9790c16d47d1>\u001b[0m in \u001b[0;36mget_quantiles\u001b[0;34m(self, feat)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m                 return np.around(\n\u001b[1;32m    186\u001b[0m                     np.unique(\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'quantiles' is not defined"
     ]
    }
   ],
   "source": [
    "f3t = ForestForTheTrees()\n",
    "f3t.load_dataset(\"bike\")\n",
    "f3t.build_base_model(300, \"gradient boosting\")\n",
    "f3t.extract_components(40, True, False)\n",
    "f3t.explain(5000, .95, \"simple\")\n",
    "f3t.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
