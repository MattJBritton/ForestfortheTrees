{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from __future__ import division\n",
    "import time\n",
    "import datetime\n",
    "import copy\n",
    "from itertools import product\n",
    "import operator\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "\n",
    "import altair as alt\n",
    "alt.renderers.enable(\"notebook\")\n",
    "import ipywidgets as wid\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForestForTheTrees:\n",
    "    \n",
    "    DEFAULT_LEARNING_RATE = 1.\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.dataset = None\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        self.feature_names = None\n",
    "        self.feature_locs = None\n",
    "        self.feature_ranges = {}\n",
    "        self.target_type = None\n",
    "        self.classifier_type = None\n",
    "        self.classifier = None\n",
    "        self.mean_prediction = None\n",
    "        self.no_predictor_features = []\n",
    "        self.oned_features = []   \n",
    "        self.binned_data = None\n",
    "        self.sample_size = None\n",
    "        self.num_tiles = None\n",
    "        self.quantiles = None\n",
    "        self.learning_rate = self.DEFAULT_LEARNING_RATE\n",
    "        self.predictions_base = None\n",
    "        self.chart_components = {}\n",
    "        self.explanation_components = {}\n",
    "        self.base_explanation = []\n",
    "        self.evaluation_details = []\n",
    "        self.base_components = []\n",
    "        self.explanation = []\n",
    "        self.cache = {}\n",
    "        \n",
    "    def __init__(self, dataset, sample_size, num_tiles, quantiles, learning_rate):\n",
    "        \n",
    "        self.classifier_type = None\n",
    "        self.classifier = None\n",
    "        self.mean_prediction = None\n",
    "        self.no_predictor_features = []\n",
    "        self.oned_features = []   \n",
    "        self.binned_data = None\n",
    "        self.sample_size = sample_size #may be set to none here, will be handled in load_dataset()\n",
    "        self.num_tiles = num_tiles\n",
    "        self.quantiles = quantiles\n",
    "        self.learning_rate = learning_rate\n",
    "        self.predictions_base = None\n",
    "        self.chart_components = {}\n",
    "        self.explanation_components = {}\n",
    "        self.base_explanation = []\n",
    "        self.evaluation_details = []\n",
    "        self.base_components = []\n",
    "        self.explanation = []\n",
    "        self.cache = {}\n",
    "        self.load_dataset(dataset)\n",
    "        \n",
    "    def set_sample_size(self, new_size):\n",
    "        self.sample_size = new_size\n",
    "    \n",
    "    def get_dataset(self, dataset):\n",
    "        \n",
    "        if dataset == \"bike\":\n",
    "            def _datestr_to_timestamp(s):\n",
    "                return time.mktime(datetime.datetime.strptime(s, \"%Y-%m-%d\").timetuple())\n",
    "\n",
    "            dataLoad = pd.read_csv('data/bike.csv')\n",
    "            dataLoad['dteday'] = dataLoad['dteday'].apply(_datestr_to_timestamp)\n",
    "            dataLoad = pd.get_dummies(dataLoad, prefix=[\"weathersit\"], columns=[\"weathersit\"], drop_first = False)\n",
    "\n",
    "            #de-normalize data to produce human-readable features.\n",
    "            #Original range info from http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\n",
    "            dataLoad[\"hum\"] = dataLoad[\"hum\"].apply(lambda x: x*100.)\n",
    "            dataLoad[\"windspeed\"] = dataLoad[\"windspeed\"].apply(lambda x: x*67.)\n",
    "            #convert Celsius to Fahrenheit\n",
    "            dataLoad[\"temp\"] = dataLoad[\"temp\"].apply(lambda x: (x*47. - 8)*9/5 +32)\n",
    "            dataLoad[\"atemp\"] = dataLoad[\"atemp\"].apply(lambda x: (x*66. - 16)*9/5 + 32)\n",
    "\n",
    "            #rename features to make them interpretable for novice users\n",
    "            feature_names_dict = {\n",
    "                \"yr\":\"First or Second Year\", \n",
    "                \"season\":\"Season\", \n",
    "                \"hr\":\"Hour of Day\", \n",
    "                \"workingday\":\"Work Day\",\n",
    "                \"weathersit_2\":\"Misty Weather\",\n",
    "                \"weathersit_3\":\"Light Precipitation\",\n",
    "                \"weathersit_4\":\"Heavy Precipitation\",\n",
    "                \"temp\":\"Temperature (F)\",\n",
    "                \"atemp\":\"Feels Like (F)\",\n",
    "                \"hum\":\"Humidity\",\n",
    "                \"windspeed\":\"Wind Speed\"\n",
    "            }\n",
    "            dataLoad = dataLoad.rename(mapper=feature_names_dict,axis=1) \n",
    "            features = feature_names_dict.values()\n",
    "\n",
    "            return {\n",
    "                \"x\": dataLoad[features].values,\n",
    "                \"y\": dataLoad[\"cnt\"],\n",
    "                \"feature_names\": features,\n",
    "                \"feature_locs\": {x:i for i,x in enumerate(features)},\n",
    "                \"target_type\": \"regression\"\n",
    "            }\n",
    "\n",
    "    def bin_data(self):\n",
    "    \n",
    "        prediction_contributions = {}\n",
    "        sample_data = pd.DataFrame(\n",
    "            self.get_sample(self.x),\n",
    "            columns = self.feature_names\n",
    "        )\n",
    "        for key in self.get_feature_pairs():\n",
    "            tempH = np.digitize(\n",
    "                sample_data.loc[:,key[0]],\n",
    "                self.feature_ranges[key[0]]\n",
    "            )-1.\n",
    "            tempV = np.digitize(\n",
    "                sample_data.loc[:,key[1]],\n",
    "                self.feature_ranges[key[1]]\n",
    "            )-1.\n",
    "            prediction_contributions[key] = (tempV*len(self.feature_ranges[key[0]]) + tempH).astype(int)\n",
    "        return prediction_contributions        \n",
    "        \n",
    "    def load_dataset(self, dataset):\n",
    "\n",
    "        self.dataset = dataset\n",
    "        data = self.get_dataset(self.dataset)\n",
    "        self.x = data[\"x\"]\n",
    "        self.y = data[\"y\"]\n",
    "        self.feature_names = data[\"feature_names\"]\n",
    "        self.feature_locs = data[\"feature_locs\"]\n",
    "        self.target_type = data[\"target_type\"]\n",
    "        self.feature_ranges = {\n",
    "            feature : self.get_quantiles(feature)\n",
    "            for feature in self.feature_names\n",
    "        } \n",
    "        if self.sample_size is None:\n",
    "            self.sample_size = self.x.shape[0]\n",
    "        self.binned_data = self.bin_data()    \n",
    "            \n",
    "    def build_base_model(self, num_estimators):\n",
    "\n",
    "        self.model_type = \"regression\"\n",
    "        self.classifier_type = GradientBoostingRegressor\n",
    "\n",
    "        self.model = self.classifier_type(\n",
    "            n_estimators=num_estimators, \n",
    "            max_depth=2, \n",
    "            learning_rate = self.learning_rate\n",
    "        )\n",
    "        self.model.fit(self.x, self.y)\n",
    "        self.pred_y = self.model.predict(self.x)\n",
    "\n",
    "    def get_model_accuracy(self):\n",
    "        return r2_score(self.y, self.pred_y)    \n",
    "        \n",
    "    def _get_coordinate_matrix(self, lst, length, direction):\n",
    "        if direction==\"h\":\n",
    "            return lst*length\n",
    "        else:\n",
    "            return [item for item in lst\\\n",
    "             for i in range(length)]   \n",
    "\n",
    "    def get_quantile_matrix(self, feat1, feat2):\n",
    "        h = self._get_coordinate_matrix(\n",
    "            list(self.feature_ranges[feat1]),\n",
    "            len(self.feature_ranges[feat2]),\n",
    "            \"h\"\n",
    "        )\n",
    "        v = self._get_coordinate_matrix(\n",
    "            list(self.feature_ranges[feat2]),\n",
    "            len(self.feature_ranges[feat1]),\n",
    "            \"v\"\n",
    "        )                      \n",
    "        return h,v \n",
    "\n",
    "    def get_leaf_value(self, tree, node_position):\n",
    "        node = tree.value[node_position]\n",
    "        return node        \n",
    "\n",
    "    def get_feature_pair_key(self, feat1, feat2):\n",
    "        if self.feature_ranges[feat1].shape[0] == self.feature_ranges[feat2].shape[0]:\n",
    "            #need stable order so keys with same number of quantiles appear in only one order\n",
    "            return tuple(sorted([feat1, feat2]))\n",
    "        elif self.feature_ranges[feat1].shape[0] > self.feature_ranges[feat2].shape[0]:\n",
    "            return tuple([feat1, feat2])\n",
    "        else:\n",
    "            return tuple([feat2, feat1])        \n",
    "\n",
    "    def get_quantiles(self, feat):\n",
    "        loc = self.feature_locs[feat]\n",
    "        if np.unique(self.x[:,loc]).shape[0] < 30 or type(self.x[0,loc]) is str: #is categorical/ordinal?\n",
    "            return np.unique(self.x[:,loc])\n",
    "        else:\n",
    "            if self.quantiles:\n",
    "                return np.around(\n",
    "                    np.unique(\n",
    "                        np.quantile(\n",
    "                            a=self.x[:,loc],\n",
    "                            q=np.linspace(0, 1, self.num_tiles)\n",
    "                        )\n",
    "                    ),\n",
    "                    1)\n",
    "            else:\n",
    "                return np.around(\n",
    "                    np.linspace(\n",
    "                        np.min(self.x[:,loc]), \n",
    "                        np.max(self.x[:,loc]),\n",
    "                        self.num_tiles\n",
    "                    )\n",
    "                    ,1)  \n",
    "            \n",
    "    def reduce_to_1d(self, arr, threshold, direction):\n",
    "        if direction == \"h\":\n",
    "            reduced_arr = arr - arr[:,0].reshape(-1,1)\n",
    "        else:\n",
    "            reduced_arr = arr - arr[0,:].reshape(1,-1)\n",
    "        return (np.max(np.abs(reduced_arr))/np.max(np.abs(arr))) <= threshold               \n",
    "        \n",
    "    def get_sample(self, arr):\n",
    "        return arr[:self.sample_size]\n",
    "    \n",
    "    def get_predictions_base(self):\n",
    "        return np.full((self.sample_size,1), np.mean(self.y))\n",
    "    \n",
    "    def get_empty_sample(self, size = None):\n",
    "        return np.full((self.sample_size if size is None else size,1), 0)\n",
    "    \n",
    "    def get_explanation_accuracy(self, explanation_predictions):\n",
    "        return r2_score(self.get_sample(self.pred_y), explanation_predictions)\n",
    "        \n",
    "    def get_prediction_contributions(self, chart, data_positions):\n",
    "        return np.take(chart, data_positions)\n",
    "    \n",
    "    def sum_arrays(self, temp_outputs, keyMain, keyAdd, arr_to_add):\n",
    "        return temp_outputs[keyMain][\"output\"]\\\n",
    "    + temp_outputs[keyAdd][arr_to_add].reshape(\n",
    "            temp_outputs[keyMain][\"output\"].shape[0]\n",
    "            if(keyMain[1]==keyAdd[1] or keyMain[1]==keyAdd[0])\n",
    "            else 1,-1\n",
    "        )\n",
    "    \n",
    "    def _drop_alternate_outputs(self,component):\n",
    "        return {\"output\": component[\"output\"]}\n",
    "    \n",
    "    def _get_prediction_contributions_df(self, components, explanation):\n",
    "        return np.hstack(\n",
    "            tuple(\n",
    "                [\n",
    "                    self.get_prediction_contributions(\n",
    "                        components[expKey][\"output\"],\n",
    "                        self.binned_data[expKey]\n",
    "                    ).reshape(-1, 1)\\\n",
    "                    for expKey in explanation                    \n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def _get_prediction_contributions_by_key(self, components, explanation):\n",
    "        return {\n",
    "            expKey : \n",
    "            self.get_prediction_contributions(\n",
    "                components[expKey][\"output\"],\n",
    "                self.binned_data[expKey]\n",
    "            ) for expKey in explanation\n",
    "        }        \n",
    "\n",
    "    def evaluate_single_explanation(self, components, explanation):\n",
    "        \n",
    "        return self.get_explanation_accuracy(\n",
    "            self.predictions_base +\\\n",
    "            np.sum(\n",
    "                np.array(\n",
    "                    self._get_prediction_contributions_by_key(\n",
    "                        components,\n",
    "                        explanation\n",
    "                    ).values()\n",
    "                ), \n",
    "                axis = 0\n",
    "            ).reshape(-1,1)\n",
    "        )\n",
    "    \n",
    "    def _get_parallel_coordinate_columns(self, explanation, cumulative):\n",
    "        #make these strings because Altair doesn't like a tuple as a key and turns it into a list\n",
    "        return ([\"mean y\"] if cumulative else [])\\\n",
    "    + [x[0] + \",\" + x[1] for x in explanation]\\\n",
    "    + ([\"prediction\"] if cumulative else [])\n",
    "    \n",
    "    def _get_altair_data_type(self,feature_name, abbreviation = True):\n",
    "        if self.feature_ranges[feature_name].shape[0] == self.num_tiles:\n",
    "            return \"Q\" if abbreviation else \"quantitative\"\n",
    "        else:\n",
    "            return \"O\" if abbreviation else \"ordinal\"    \n",
    "    \n",
    "    def _get_datapoint_contributions(self, components, explanation):\n",
    "        contributions = self._get_prediction_contributions_df(components, explanation)\n",
    "        #raw contributions\n",
    "        arr = np.hstack(\n",
    "            (\n",
    "                self.get_empty_sample().reshape(-1,1),\n",
    "                contributions.reshape(self.sample_size, -1),\n",
    "                self.get_sample(self.pred_y).reshape(-1,1),\n",
    "                np.array([0. for x in range(self.sample_size)]).reshape(-1,1)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        #cumulative version\n",
    "        arr_cum = np.cumsum(\n",
    "            np.hstack(\n",
    "                (\n",
    "                    self.get_predictions_base().reshape(-1,1),\n",
    "                    contributions.reshape(self.sample_size,-1)\n",
    "                )\n",
    "            ),\n",
    "            axis = 1\n",
    "        )\n",
    "        \n",
    "        arr_cum = np.hstack(\n",
    "            (\n",
    "                arr_cum,\n",
    "                self.get_sample(self.pred_y).reshape(-1,1),\n",
    "                np.array([1. for x in range(self.sample_size)]).reshape(-1,1)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        arr_df = pd.DataFrame(\n",
    "            arr,\n",
    "            columns = self._get_parallel_coordinate_columns(explanation, True) + [\"view\"]\n",
    "        )\n",
    "        \n",
    "        #combine arrays\n",
    "        arr_cum_df = pd.DataFrame(\n",
    "            arr_cum,\n",
    "            columns = self._get_parallel_coordinate_columns(explanation, True) + [\"view\"]\n",
    "        )\n",
    "        \n",
    "        #generate datapoint id columns\n",
    "        arr_df = arr_df.reset_index(drop = False)\n",
    "        arr_cum_df = arr_cum_df.reset_index(drop = False)\n",
    "        datapoints = pd.concat([arr_df, arr_cum_df])\n",
    "        \n",
    "        #couldn't do this earlier, as you can't vstack a mixed type array\n",
    "        datapoints[\"view\"] = datapoints[\"view\"].apply(lambda x:\\\n",
    "                                                      'Predictions by Chart'\\\n",
    "                                                      if x < 1. else\\\n",
    "                                                      'Cumulative Predictions'\\\n",
    "                                                     )\n",
    "        #calculate explanation loss\n",
    "        datapoints[\"explanation_loss\"] = np.abs( #otherwise the chart is hard to read with 0 in the middle of the axis\n",
    "            datapoints.loc[:,\"prediction\"] - datapoints.iloc[:,-3]#last cumulative column\n",
    "        )\n",
    "        \n",
    "        #datapoints = datapoints.reset_index(drop = False)\n",
    "        datapoints[\"prediction_index\"] = datapoints[\"prediction\"]\n",
    "        datapoints = datapoints.melt(\n",
    "            id_vars = ['index', \"prediction_index\", \"view\", \"explanation_loss\"],\n",
    "            var_name = 'component',\n",
    "            value_name = 'contribution'\n",
    "        )\n",
    "        \n",
    "        #rename prediction again\n",
    "        datapoints = datapoints.rename({\"prediction_index\" : \"prediction\"}, axis = 1)\n",
    "\n",
    "        #drop fake columns for \"Predictions by Chart\"\n",
    "        datapoints = datapoints[\n",
    "            (datapoints[\"view\"] == \"Cumulative Predictions\")\n",
    "            | (~datapoints[\"component\"].isin([\"prediction\", \"mean y\", \"explanation_loss\"]))\n",
    "        ]\n",
    "        \n",
    "        #build sort column for Altair\n",
    "        datapoints[\"sort\"] = datapoints[\"component\"].apply(lambda x:\n",
    "                                                           self._get_parallel_coordinate_columns(\n",
    "                                                               explanation,\n",
    "                                                               True\n",
    "                                                           ).index(x)\n",
    "                                                          )   \n",
    "        return datapoints\n",
    "\n",
    "    def copy_chart_components(self):\n",
    "        return copy.deepcopy(self.chart_components)  \n",
    "    \n",
    "    def get_feature_pairs(self):\n",
    "        return [\n",
    "            self.get_feature_pair_key(key[0], key[1])\n",
    "            for key in [tuple(t) for t in product(self.feature_names, repeat = 2)]\n",
    "        ]       \n",
    "\n",
    "    def rollup_components(self, explanation):\n",
    "        temp_outputs = self.copy_chart_components()\n",
    "        for keyRollup in [k for k in self.chart_components.iterkeys() if k not in explanation]:\n",
    "            hUsed = False\n",
    "            vUsed = False\n",
    "            for keyExisting in explanation:\n",
    "                if (keyRollup[1] == keyExisting[0] or keyRollup[1] == keyExisting[1]) and not hUsed:\n",
    "                    hUsed = True\n",
    "                    if vUsed:\n",
    "                        temp_outputs[keyExisting][\"output\"] = self.sum_arrays(\n",
    "                            temp_outputs,\n",
    "                            keyExisting, \n",
    "                            keyRollup,\n",
    "                            \"output_HReduced\"\n",
    "                        )\n",
    "                        break\n",
    "                    else:\n",
    "                        temp_outputs[keyExisting][\"output\"] = self.sum_arrays(\n",
    "                            temp_outputs,\n",
    "                            keyExisting, \n",
    "                            keyRollup,\n",
    "                            \"output_H\"\n",
    "                        )                           \n",
    "                elif (keyRollup[0] == keyExisting[0] or keyRollup[0] == keyExisting[1]) and not vUsed:\n",
    "                    vUsed = True\n",
    "                    if hUsed:\n",
    "                        temp_outputs[keyExisting][\"output\"] = self.sum_arrays(\n",
    "                            temp_outputs,\n",
    "                            keyExisting, \n",
    "                            keyRollup,\n",
    "                            \"output_VReduced\"\n",
    "                        )                          \n",
    "                        break\n",
    "                    else:\n",
    "                        temp_outputs[keyExisting][\"output\"] = self.sum_arrays(\n",
    "                            temp_outputs,\n",
    "                            keyExisting, \n",
    "                            keyRollup,\n",
    "                            \"output_V\"\n",
    "                        )  \n",
    "        return temp_outputs   \n",
    "    \n",
    "    def visualize_single_estimator(self, estimator_num):\n",
    "        \n",
    "        chart_components,\\\n",
    "        chart_indices,\\\n",
    "        _,\\\n",
    "        _,\\\n",
    "        function_texts = self._extract_components(False, [self.model.estimators_[estimator_num]], True)\n",
    "        \n",
    "        print function_texts\n",
    "        chart = self._visualize_components(\n",
    "            chart_components.keys(),\n",
    "            chart_components,\n",
    "            None,\n",
    "            chart_indices,\n",
    "            False, \n",
    "            300\n",
    "        )\n",
    "        display(chart)\n",
    "        return chart\n",
    "    \n",
    "    def _get_function_text(self, decision_func_dict):\n",
    "        \n",
    "        def _get_left_right_text(op, le, gt):\n",
    "            if op == operator.le:\n",
    "                return \" is less than or equal to \", str(round(le,1)), str(round(gt,1))\n",
    "            else:\n",
    "                return \" is greater than \", str(gt), str(le)\n",
    "                \n",
    "        \n",
    "        if \"feature_name\" in decision_func_dict: #1-deep\n",
    "            \n",
    "            comparison_text, left, right = _get_left_right_text(\n",
    "                decision_func_dict[\"operator\"],\n",
    "                decision_func_dict[\"prob_le\"],\n",
    "                decision_func_dict[\"prob_gt\"]\n",
    "            )\n",
    "            \n",
    "            text = \"If \" + decision_func_dict[\"feature_name\"] + comparison_text\\\n",
    "            + str(round(decision_func_dict[\"threshold\"],1)) + \" then \" + left\\\n",
    "            + \" else \" + right + \". \"\n",
    "            \n",
    "            return text\n",
    "        \n",
    "        else: #2-deep\n",
    "            comparison_text_1, left_1, right_1 = _get_left_right_text(\n",
    "                decision_func_dict[\"operator_1\"],\n",
    "                0,\n",
    "                0\n",
    "            )\n",
    "\n",
    "            comparison_text_2, left_2, right_2 = _get_left_right_text(\n",
    "                decision_func_dict[\"operator_2\"],\n",
    "                decision_func_dict[\"prob_le\"],\n",
    "                decision_func_dict[\"prob_gt\"]\n",
    "            ) \n",
    "            \n",
    "            \n",
    "            text = \"If \" + decision_func_dict[\"feature_name_1\"] + comparison_text_1\\\n",
    "            + str(round(decision_func_dict[\"threshold_1\"],1)) + \" then proceed. If \"\\\n",
    "            + decision_func_dict[\"feature_name_2\"] + comparison_text_2\\\n",
    "            + str(round(decision_func_dict[\"threshold_2\"],1)) + \" then \" + left_2 + \" else \" + right_2 + \". \"\n",
    "            \n",
    "            return text\n",
    "    \n",
    "    def extract_components(self, collapse_1d = True, return_text = False):\n",
    "        \n",
    "        self.chart_components,\\\n",
    "        self.chart_indices,\\\n",
    "        self.no_predictor_features,\\\n",
    "        self.oned_features,\\\n",
    "        self.estimator_texts = self._extract_components(collapse_1d, self.model.estimators_, return_text)\n",
    "        \n",
    "        self.predictions_base = self.get_predictions_base()\n",
    "        \n",
    "        #get the full explanation and store it. one reason for this is so that\n",
    "        #charts can also be sorted appropriately\n",
    "        #don't save explanation components as they will be the same as chart_components\n",
    "        self.base_explanation, _, _\\\n",
    "        = self._explain(1., None)        \n",
    "        \n",
    "    def _extract_components(self, collapse_1d, estimators, return_text):\n",
    "\n",
    "        #generate data structure for pairwise charts\n",
    "        feature_pairs = {\n",
    "            key : {\n",
    "                \"map\":None,\n",
    "                \"predicates\":[],\n",
    "                \"function_texts\":[]\n",
    "            }\n",
    "            for key in self.get_feature_pairs()\n",
    "        }      \n",
    "\n",
    "        for key, value in feature_pairs.iteritems():\n",
    "            h, v = self.get_quantile_matrix(key[0], key[1])\n",
    "            value[\"map\"] = np.array(\n",
    "                [\n",
    "                    {\n",
    "                        key[0] : x,\n",
    "                        key[1] : y\n",
    "                    }\n",
    "                    for x,y in zip(h,v)\n",
    "                ]\n",
    "            ).reshape(len(self.feature_ranges[key[1]]), len(self.feature_ranges[key[0]]))\n",
    "\n",
    "        for modelT in estimators:\n",
    "            curr_model = modelT[0]\n",
    "            feature_ids = {\n",
    "                i : {\n",
    "                    \"number\":x,\n",
    "                    \"name\":self.feature_names[x]\n",
    "                } for i,x in enumerate(list(curr_model.tree_.feature))\n",
    "                if x >= 0\n",
    "            } #-2 means leaf node\n",
    "\n",
    "            #for 1-layer trees\n",
    "            if curr_model.tree_.feature[1] < 0:\n",
    "                feature_pair_key = self.get_feature_pair_key(\n",
    "                    feature_ids[0][\"name\"],\n",
    "                    feature_ids[0][\"name\"]\n",
    "                )\n",
    "                decision_func_dict = {\n",
    "                    \"feature_name\": feature_ids[0][\"name\"],\n",
    "                    \"threshold\": curr_model.tree_.threshold[0],\n",
    "                    \"operator\": operator.le,\n",
    "                    \"prob_le\": self.get_leaf_value(curr_model.tree_, 1),\n",
    "                    \"prob_gt\": self.get_leaf_value(curr_model.tree_, 2)\n",
    "                }       \n",
    "                #build the predictive function used in the decision tree\n",
    "                def dt_predicate(data_case, decision_func_dict=decision_func_dict):\n",
    "                    if decision_func_dict[\"operator\"](\\\n",
    "                                                        data_case[decision_func_dict[\"feature_name\"]],\\\n",
    "                                                        decision_func_dict[\"threshold\"]\\\n",
    "                                                       ):\n",
    "                        return decision_func_dict[\"prob_le\"]\n",
    "                    else:\n",
    "                        return decision_func_dict[\"prob_gt\"]        \n",
    "            else:\n",
    "                for node_position in [1,4]: #positions for left and right nodes at layer 2\n",
    "                    if node_position in feature_ids:\n",
    "                        feature_pair_key = self.get_feature_pair_key(\n",
    "                            feature_ids[0][\"name\"], \n",
    "                            feature_ids[node_position][\"name\"]\n",
    "                        )\n",
    "                        #get the decision rules\n",
    "                        decision_func_dict = {\n",
    "                            \"feature_name_1\": feature_ids[0][\"name\"],\n",
    "                            \"threshold_1\": curr_model.tree_.threshold[0],\n",
    "                            \"operator_1\": operator.le if node_position == 1 else operator.gt,\n",
    "\n",
    "                            \"feature_name_2\": feature_ids[node_position][\"name\"],\n",
    "                            \"threshold_2\": curr_model.tree_.threshold[node_position],\n",
    "                            \"operator_2\": operator.le,\n",
    "\n",
    "                            \"prob_le\": self.get_leaf_value(curr_model.tree_, node_position+1),\n",
    "                            \"prob_gt\": self.get_leaf_value(curr_model.tree_, node_position+2)\n",
    "                        }\n",
    "                        #build the predictive function used in the decision tree\n",
    "                        def dt_predicate(data_case, decision_func_dict=decision_func_dict):\n",
    "                            if decision_func_dict[\"operator_1\"](\\\n",
    "                                                                data_case[decision_func_dict[\"feature_name_1\"]],\\\n",
    "                                                                decision_func_dict[\"threshold_1\"]\\\n",
    "                                                               ):\n",
    "                                if decision_func_dict[\"operator_2\"](\\\n",
    "                                                                    data_case[decision_func_dict[\"feature_name_2\"]],\\\n",
    "                                                                    decision_func_dict[\"threshold_2\"]\\\n",
    "                                                                   ):\n",
    "                                    return decision_func_dict[\"prob_le\"]\n",
    "                                else:\n",
    "                                    return decision_func_dict[\"prob_gt\"]\n",
    "                            else:\n",
    "                                return 0.\n",
    "\n",
    "                    else: #asymmetric tree, this is a leaf node\n",
    "                        feature_pair_key = self.get_feature_pair_key(\n",
    "                            feature_ids[0][\"name\"], \n",
    "                            feature_ids[0][\"name\"]\n",
    "                        )\n",
    "                        decision_func_dict = {\n",
    "                            \"feature_name\": feature_ids[0][\"name\"],\n",
    "                            \"threshold\": curr_model.tree_.threshold[0],\n",
    "                            \"operator\": operator.le if node_position == 1 else operator.gt,\n",
    "                            \"prob\": curr_model.tree_.value[node_position]\n",
    "                        }\n",
    "                        #build the predictive function used in the decision tree\n",
    "                        def dt_predicate(data_case, decision_func_dict=decision_func_dict):\n",
    "                            if decision_func_dict[\"operator\"](\\\n",
    "                                                                data_case[decision_func_dict[\"feature_name\"]],\\\n",
    "                                                                decision_func_dict[\"threshold\"]\\\n",
    "                                                               ):\n",
    "                                return decision_func_dict[\"prob\"]\n",
    "                            else:                         \n",
    "                                return 0.                 \n",
    "\n",
    "                    feature_pairs[feature_pair_key][\"predicates\"].append(dt_predicate)\n",
    "                    if return_text:\n",
    "                        feature_pairs[feature_pair_key][\"function_texts\"].append(\n",
    "                            self._get_function_text(\n",
    "                                decision_func_dict\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "        #now calculate output array for each feature pair\n",
    "        for key, value in feature_pairs.iteritems():\n",
    "            arrs = []\n",
    "            for predicate in value[\"predicates\"]:\n",
    "                f = np.vectorize(predicate)\n",
    "                arrs.append(f(value[\"map\"]))\n",
    "            if len(arrs) > 0:\n",
    "                #details of vote aggreggation method for random forest\n",
    "                #https://stats.stackexchange.com/questions/127077/random-forest-probabilistic-prediction-vs-majority-vote\n",
    "                value[\"output\"] = np.sum(np.stack(arrs, axis=-1), axis=-1)*self.learning_rate \n",
    "            else:\n",
    "                value[\"output\"] = None\n",
    "\n",
    "        #build chart data\n",
    "        for key, value in feature_pairs.iteritems():\n",
    "            h,v = self.get_quantile_matrix(key[0], key[1])\n",
    "            value[\"h_indices\"] = h\n",
    "            value[\"v_indices\"] = v    \n",
    "\n",
    "        no_predictor_features = []\n",
    "        oned_features = []\n",
    "        chart_data = {}\n",
    "        for key, value in feature_pairs.iteritems(): \n",
    "            newKey = key\n",
    "            if value[\"output\"] is None:\n",
    "                no_predictor_features.append(key)\n",
    "                value[\"removed\"] = True\n",
    "            else:          \n",
    "                if collapse_1d:\n",
    "                    if self.reduce_to_1d(value[\"output\"], 0., \"v\"):\n",
    "                        newKey = key[1]\n",
    "                        value[\"output\"] = value[\"output\"][0,:]\n",
    "                        value[\"h_indices\"] = self.feature_ranges[newKey]\n",
    "                        value[\"v_indices\"] = None\n",
    "                        value[\"1d_key\"] = newKey\n",
    "                        value[\"removed\"] = True\n",
    "                        oned_features.append(key)                 \n",
    "                    elif self.reduce_to_1d(value[\"output\"], 0., \"h\"):\n",
    "                        newKey = key[0]\n",
    "                        value[\"output\"] = value[\"output\"][:,0]\n",
    "                        value[\"h_indices\"] = self.feature_ranges[newKey]\n",
    "                        value[\"v_indices\"] = None\n",
    "                        value[\"1d_key\"] = newKey\n",
    "                        value[\"removed\"] = True\n",
    "                        oned_features.append(key)\n",
    "\n",
    "        #do another loop through chart_data to push 1d charts into 2d\n",
    "        if collapse_1d:\n",
    "            for value in feature_pairs.itervalues():\n",
    "                if value[\"v_indices\"] is None:\n",
    "                    key = value[\"1d_key\"]\n",
    "                    #get list of charts with this feature\n",
    "                    matchList = sorted([{\"key\": kInner, \"feature_importance\": np.std(vInner[\"output\"])}\\\n",
    "                                        for kInner, vInner in feature_pairs.iteritems()\\\n",
    "                                        if \"removed\" not in vInner and key in kInner],\\\n",
    "                                       key=lambda x: x[\"feature_importance\"], reverse=True)\n",
    "\n",
    "                    if len(matchList) > 0:\n",
    "                        matchKey = matchList[0][\"key\"]\n",
    "                        feature_pairs[matchKey][\"output\"] = feature_pairs[matchKey][\"output\"]\\\n",
    "                        + value[\"output\"].reshape(\\\n",
    "                                                  -1 if key == matchKey[1] else 1,\\\n",
    "                                                  -1 if key == matchKey[0] else 1\\\n",
    "                                                 )\n",
    "\n",
    "        #one last loop to generate the horizontal and vertical components\n",
    "        for key, value in feature_pairs.iteritems():\n",
    "            if \"removed\" in value:\n",
    "                pass\n",
    "            else:\n",
    "                value[\"output_H\"] = np.mean(value[\"output\"], axis=1).reshape(-1,1)\n",
    "                value[\"output_V\"] = np.mean(value[\"output\"], axis=0).reshape(1,-1)\n",
    "                value[\"output_HReduced\"] = np.mean(value[\"output\"] - value[\"output_V\"].reshape(1,-1), axis=1)\\\n",
    "                .reshape(1,-1)\n",
    "                value[\"output_VReduced\"] = np.mean(value[\"output\"] - value[\"output_H\"].reshape(-1,1), axis=0)\\\n",
    "                .reshape(-1,1)\n",
    "\n",
    "        #remove deleted keys\n",
    "        feature_pairs = {key:val for key, val in feature_pairs.iteritems() if \"removed\" not in val}\n",
    "        feature_pairs = OrderedDict(sorted(feature_pairs.items(),\\\n",
    "                                            key=lambda x: np.std(x[1][\"output\"]), reverse=True))\n",
    "        chart_components = {\n",
    "            key: {\n",
    "                \"output\" : val[\"output\"],\n",
    "                \"output_VReduced\" : val[\"output_VReduced\"],\n",
    "                \"output_H\" : val[\"output_H\"],\n",
    "                \"output_HReduced\" : val[\"output_HReduced\"],\n",
    "                \"output_V\" : val[\"output_V\"]\n",
    "            } for key, val in feature_pairs.iteritems()\n",
    "        }\n",
    "\n",
    "        chart_indices = {\n",
    "            key: {\n",
    "                \"h_indices\" : val[\"h_indices\"],\n",
    "                \"v_indices\" : val[\"v_indices\"]\n",
    "            } for key, val in feature_pairs.iteritems()\n",
    "        }\n",
    "        \n",
    "        function_texts = {\n",
    "            key : {\n",
    "                \"function_texts\" : val[\"function_texts\"]\n",
    "            } for key, val in feature_pairs.iteritems()\n",
    "        } if return_text else None\n",
    "        \n",
    "        return chart_components, chart_indices, no_predictor_features, oned_features, function_texts\n",
    "        \n",
    "    def _explain(self, fidelity_threshold = 1., rollup = None):\n",
    "\n",
    "        explanation = []   \n",
    "        explanation_components = {}\n",
    "        evaluation_details = [\n",
    "            {\n",
    "                \"score\": self.get_explanation_accuracy(\n",
    "                    self.predictions_base\n",
    "                )\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        while evaluation_details[-1][\"score\"] < fidelity_threshold\\\n",
    "        and len(explanation) < len(self.chart_components):\n",
    "            current_details = {}\n",
    "            temp_outputs = {}\n",
    "            keys_to_evaluate = [key for key in self.chart_components.iterkeys() if key not in explanation]\n",
    "            for key in keys_to_evaluate:\n",
    "                #roll up other keys\n",
    "                current_explanation = explanation+[key]\n",
    "                temp_outputs[key] = self.rollup_components(current_explanation)\\\n",
    "                if rollup == \"advanced\"\\\n",
    "                else self.copy_chart_components()\n",
    "\n",
    "                current_details[key] = self.evaluate_single_explanation(temp_outputs[key], current_explanation)\n",
    "\n",
    "            #get key with highest fidelity score\n",
    "            best_key = max(\n",
    "                current_details.iterkeys(),\\\n",
    "                key = (lambda key: current_details[key])\n",
    "            )\n",
    "            explanation.append(best_key)\n",
    "            current_details[\"best_key\"] = best_key\n",
    "\n",
    "            if rollup == \"simple\":\n",
    "                temp_outputs[best_key] = self.rollup_components(explanation)\n",
    "                current_details[best_key] = self.evaluate_single_explanation(temp_outputs[best_key], explanation)\n",
    "\n",
    "            \n",
    "            current_details[\"score\"] = current_details[best_key]\n",
    "            evaluation_details.append(current_details)\n",
    "            explanation_components = {k : self._drop_alternate_outputs(v)\\\n",
    "                                      for k, v in temp_outputs[best_key].iteritems()}\n",
    "        return explanation, explanation_components, evaluation_details\n",
    "    \n",
    "    def explain(self, fidelity_threshold = 1., rollup = None):\n",
    "        self.explanation, self.explanation_components, self.evaluation_details\\\n",
    "        = self._explain(fidelity_threshold, rollup)\n",
    "        \n",
    "    def cache_visualize_components(self, start = 1, end = 100, step = 1):\n",
    "        self.cache[\"play_components\"] = []\n",
    "        for i in range(start, end+1, step):\n",
    "            ft = ForestForTheTrees(\n",
    "                dataset = self.dataset,\n",
    "                sample_size = self.sample_size,\n",
    "                num_tiles = self.num_tiles,\n",
    "                quantiles = self.quantiles,\n",
    "                learning_rate = self.learning_rate\n",
    "            )\n",
    "            ft.build_base_model(i)\n",
    "            ft.extract_components(True)\n",
    "            self.cache[\"play_components\"].append(\n",
    "                {\n",
    "                    \"explanation\" : ft.base_explanation,\n",
    "                    \"components\" : ft.chart_components,\n",
    "                    \"chart_indices\" : ft.chart_indices\n",
    "                }\n",
    "            )\n",
    "        \n",
    "    def cache_visualize_datapoints(self):\n",
    "        \n",
    "        minimal = self._get_datapoint_contributions(\n",
    "            self.explanation_components,\n",
    "            self.explanation\n",
    "        )\n",
    "        \n",
    "        full = self._get_datapoint_contributions(\n",
    "            self.explanation_components,\n",
    "            self.base_explanation\n",
    "        )\n",
    "        \n",
    "        minimal[\"explanation\"] = \"minimal\"\n",
    "        full[\"explanation\"] = \"full\"\n",
    "        \n",
    "        self.cache[\"datapoints\"] = pd.concat([minimal, full])\n",
    "        \n",
    "    def visualize_datapoints(self, cumulative = False, num_datapoints = 50, explanation_type = \"minimal\",\n",
    "                            color_encoding = \"prediction\"):\n",
    "        output = self._visualize_datapoints(explanation_type, cumulative, num_datapoints, color_encoding)\n",
    "        display(output)\n",
    "        return output\n",
    "    \n",
    "    def _visualize_datapoints(self, explanation_type, cumulative, num_datapoints, color_encoding):\n",
    "        \n",
    "        explanation_to_visualize = self.explanation\\\n",
    "        if len(self.explanation) > 0 and explanation_type == \"minimal\"\\\n",
    "        else self.base_explanation        \n",
    "        \n",
    "        if \"datapoints\" in self.cache and self.cache[\"datapoints\"] is not None:\n",
    "            datapoints = self.cache[\"datapoints\"]\n",
    "            datapoints = datapoints[datapoints[\"explanation\"] == explanation_type]\n",
    "        \n",
    "        else:            \n",
    "            datapoints = self._get_datapoint_contributions(\n",
    "                self.explanation_components,\n",
    "                explanation_to_visualize\n",
    "            )\n",
    "        \n",
    "        unique_datapoint_ids = np.unique(datapoints.loc[:,\"index\"])\n",
    "        sample_datapoint_ids = np.random.choice(unique_datapoint_ids, num_datapoints, replace = False)\n",
    "        datapoints = datapoints[datapoints[\"index\"].isin(sample_datapoint_ids)]\n",
    "        datapoints = datapoints[datapoints[\"view\"] == (\"Cumulative Predictions\"\\\n",
    "                                                       if cumulative\\\n",
    "                                                       else \"Predictions by Chart\")\n",
    "                               ]\n",
    "        \n",
    "        df_raw = pd.DataFrame(self.x, columns = self.feature_names)\n",
    "        df_raw[\"prediction\"] = self.pred_y\n",
    "        df_raw = df_raw.reset_index(drop = False)\n",
    "        df_raw = df_raw[df_raw[\"index\"].isin(sample_datapoint_ids)]       \n",
    "        \n",
    "        brush = alt.selection_multi()\n",
    "        chart = alt.Chart(data = datapoints)\\\n",
    "        .mark_line()\\\n",
    "        .encode(\n",
    "            x = alt.X(\n",
    "                field = 'component',\n",
    "                type = 'nominal',\n",
    "                axis = alt.Axis(labelAngle = -30),\n",
    "                sort = self._get_parallel_coordinate_columns(explanation_to_visualize, cumulative)\n",
    "            ),\n",
    "            y ='contribution:Q',\n",
    "            color = alt.condition(\n",
    "                brush,\n",
    "                alt.Color(\n",
    "                    field = color_encoding,\n",
    "                    type = \"quantitative\",\n",
    "                    scale = alt.Scale(scheme = \"plasma\")\n",
    "                ),\n",
    "                alt.value(\"lightgray\")\n",
    "            ),\n",
    "            opacity = alt.condition(\n",
    "                brush,\n",
    "                alt.value(1.0),\n",
    "                alt.value(0.2)\n",
    "            ),\n",
    "            tooltip = [\n",
    "                alt.Tooltip(x+\":\"+self._get_altair_data_type(x))\n",
    "                for x in self.feature_names\n",
    "            ] + [\n",
    "                alt.Tooltip(x+\":Q\")\n",
    "                for x in [\"prediction\", \"explanation_loss\"]                \n",
    "            ],         \n",
    "            detail = 'index:N',\n",
    "            order = \"sort:N\"\n",
    "        ).transform_lookup(\n",
    "            lookup = 'index',\n",
    "            from_ = alt.LookupData(\n",
    "                data = df_raw, \n",
    "                key = 'index',\n",
    "                fields = self.feature_names\n",
    "            )\n",
    "        ).properties(\n",
    "            height = 300,\n",
    "            width = 800\n",
    "        ).add_selection(\n",
    "            brush\n",
    "        )\n",
    "        \n",
    "        return chart\n",
    "        \n",
    "    def visualize_components(self, plot_points = True, chart_size = 150):\n",
    "        if len(self.explanation) > 0:\n",
    "            explanation_to_visualize = self.explanation\n",
    "            components = self.explanation_components\n",
    "        else:\n",
    "            explanation_to_visualize = self.base_explanation\n",
    "            components = self.chart_components\n",
    "        return self._visualize_components(\n",
    "            explanation_to_visualize,\n",
    "            components,\n",
    "            None,\n",
    "            self.chart_indices,\n",
    "            plot_points,\n",
    "            chart_size\n",
    "        )\n",
    "    \n",
    "    def play_components(self, cache_id):\n",
    "        output = self._visualize_components(\n",
    "            #-1 deals with the fact that list is zero-based but number of trees starts at 1\n",
    "            self.cache[\"play_components\"][cache_id-1][\"explanation\"],\n",
    "            self.cache[\"play_components\"][cache_id-1][\"components\"],\n",
    "            self.cache[\"play_components\"][cache_id-2][\"components\"] if cache_id > 1 else None,\n",
    "            self.cache[\"play_components\"][cache_id-1][\"chart_indices\"],\n",
    "            False,\n",
    "            150\n",
    "        )\n",
    "        display(output)\n",
    "        return output\n",
    "        \n",
    "    def _visualize_components(self, explanation, components, ref_components, chart_indices,\n",
    "                              plot_points, chart_size):\n",
    "        i = 1\n",
    "        rows = []\n",
    "        charts = []\n",
    "        self.temp_components = components.copy()\n",
    "        self.temp_indices = chart_indices.copy()\n",
    "        for key in explanation:\n",
    "            \n",
    "            chart_df = pd.DataFrame(\n",
    "                np.hstack(\n",
    "                    (\n",
    "                        np.array(chart_indices[key][\"h_indices\"]).reshape(-1,1),\n",
    "                        np.array(chart_indices[key][\"v_indices\"]).reshape(-1,1),\n",
    "                        components[key][\"output\"].ravel().reshape(-1,1),\n",
    "                        \n",
    "                        ref_components[key][\"output\"].ravel().reshape(-1,1)\\\n",
    "                        if ref_components is not None and key in ref_components\\\n",
    "                        else self.get_empty_sample(len(chart_indices[key][\"h_indices\"])).reshape(-1,1)\n",
    "                    )\n",
    "                ),\n",
    "                columns = [\"h_indices\", \"v_indices\", \"contributions\", \"ref_contributions\"]\n",
    "            )\n",
    "            \n",
    "            #figure out cells that should be highlighted\n",
    "            chart_df[\"is_changed\"]\\\n",
    "            = chart_df.apply(lambda x:\n",
    "                             abs(x[\"ref_contributions\"] - x[\"contributions\"])/abs(x[\"contributions\"]+0.001) > 0.05\\\n",
    "                             or (x[\"contributions\"] != 0. and key not in ref_components)#new chart this step\n",
    "                             if ref_components is not None else False,\n",
    "                             axis = 1\n",
    "                            )\n",
    "\n",
    "            y_encoding = alt.Y(\n",
    "                field = \"v_indices\",\n",
    "                type = \"ordinal\",\n",
    "                sort = \"descending\",\n",
    "                axis = alt.Axis(title = key[1])\n",
    "            )                \n",
    "\n",
    "            x_encoding = alt.X(\n",
    "                field = \"h_indices\",\n",
    "                type = \"ordinal\",\n",
    "                sort = \"ascending\",\n",
    "                axis = alt.Axis(\n",
    "                    title = key[0],\n",
    "                    labelAngle = 0,\n",
    "                    labelOverlap = \"greedy\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "            color_encoding = alt.Color(\n",
    "                field = \"contributions\",\n",
    "                type = \"quantitative\",\n",
    "                scale = alt.Scale(\n",
    "                    scheme = \"redblue\",\n",
    "                    domain = [\n",
    "                        np.min([np.min(x[\"output\"]) for x in self.explanation_components.values()]),\n",
    "                        np.max([np.max(x[\"output\"]) for x in self.explanation_components.values()])\n",
    "                    ]\n",
    "                ),\n",
    "                legend = alt.Legend(title = \"Votes\")\n",
    "            )\n",
    "            \n",
    "            tooltip_encoding = [\n",
    "                alt.Tooltip('h_indices:O', title = key[0]),\n",
    "                alt.Tooltip('v_indices:O', title = key[1]),\n",
    "                alt.Tooltip(\"contributions:Q\", title = \"Contribution\")\n",
    "            ]\n",
    "\n",
    "            chart = alt.Chart(data = chart_df).mark_rect()\n",
    "\n",
    "            chart = chart.encode(\n",
    "                x = x_encoding, \n",
    "                y = y_encoding, \n",
    "                color = color_encoding,\n",
    "                tooltip = tooltip_encoding\n",
    "            ).properties(width = chart_size, height = chart_size)\n",
    "\n",
    "            if plot_points:\n",
    "                point_df = pd.DataFrame(self.x[np.random.choice(self.x.shape[0],500,replace = False),:],\\\n",
    "                                  columns = self.feature_names)\n",
    "                points = alt.Chart(point_df).mark_circle(\n",
    "                    color = 'black',\n",
    "                    size = 2\n",
    "                ).encode(\n",
    "                    x = alt.X(field = key[0], type = \"quantitative\", sort = \"ascending\", axis = None),\n",
    "                    y = alt.Y(field = key[1], type = \"quantitative\", sort = \"ascending\", axis = None)\n",
    "                ).properties(width = chart_size, height = chart_size)\n",
    "                chart = chart + points\n",
    "                #chart = chart.resolve_scale(x = \"independent\", y = \"independent\")\n",
    "                \n",
    "            elif ref_components is not None:\n",
    "                changes = alt.Chart(data = chart_df[chart_df[\"is_changed\"]]).mark_circle(size = 8).encode(\n",
    "                    x = alt.X(field = \"h_indices\", type = \"ordinal\", sort = \"ascending\", axis = None),\n",
    "                    y = alt.Y(field = \"v_indices\", type = \"ordinal\", sort = \"descending\", axis = None)\n",
    "                ).properties(width = chart_size, height = chart_size)\n",
    "                chart = chart + changes\n",
    "                chart = chart.resolve_scale(y = \"independent\")\n",
    "\n",
    "            charts.append(chart)\n",
    "            if len(charts) == 4 or i == len(explanation):\n",
    "                rows.append(alt.hconcat(*charts))\n",
    "                charts = []\n",
    "            i += 1\n",
    "        return alt.vconcat(*rows)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2t = ForestForTheTrees(dataset = \"bike\", sample_size = None, num_tiles = 20, quantiles = False, learning_rate = 1.)\n",
    "f2t.build_base_model(300)\n",
    "f2t.extract_components(True, False)\n",
    "f2t.explain(.95, None)\n",
    "#f2t.visualize(True, 100)\n",
    "#f2t.cache_visualize_datapoints()\n",
    "#f2t.cache_visualize_components(start = 1, end = 10, step = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2t.visualize_components()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2d9c38b7444f4dadada8d91585168e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntSlider(value=1, continuous_update=False, description=u'Tree #', max=300, min=1),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6217e65c2a9d4f9dbdcf07e9fb3d1fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "single_estimator_slider = wid.IntSlider(\n",
    "    value = 1,\n",
    "    min = 1,\n",
    "    max = 300,\n",
    "    step = 1,\n",
    "    continuous_update = False,\n",
    "    description = \"Tree #\"\n",
    ")\n",
    "ui = wid.HBox([single_estimator_slider])\n",
    "output = wid.interactive_output(\n",
    "    f2t.visualize_single_estimator,\n",
    "    {\"estimator_num\" : single_estimator_slider}\n",
    ")\n",
    "display(ui, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_control = wid.Play(\n",
    "    interval = 3000,\n",
    "    value = 1,\n",
    "    min = 1,\n",
    "    max = 10,\n",
    "    step = 1,\n",
    "    description = \"Press play\"\n",
    ")\n",
    "play_slider = wid.IntSlider(\n",
    "    value = 1,\n",
    "    min = 1,\n",
    "    max = 10,\n",
    "    step = 1,\n",
    "    continuous_update = False,\n",
    "    description = \"# of trees\"\n",
    ")\n",
    "wid.jslink((play_control, 'value'), (play_slider, 'value'))\n",
    "ui = wid.HBox([play_control, play_slider])\n",
    "output = wid.interactive_output(\n",
    "    f2t.play_components,\n",
    "    {\"cache_id\" : play_control}\n",
    ")\n",
    "display(ui, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f2t.cache_visualize_datapoints()\n",
    "cumulative_selector = wid.Dropdown(\n",
    "    options = [('Cumulative Prediction', True), ('Contributions by Feature', False)],\n",
    "    value = True,\n",
    "    description = 'View',\n",
    ")\n",
    "\n",
    "num_datapoints_slider = wid.IntSlider(\n",
    "    value = 50,\n",
    "    min = 1,\n",
    "    max = 500,\n",
    "    step = 5,\n",
    "    description = '# datapoints',\n",
    "    continuous_update = False,\n",
    "    orientation = 'horizontal',\n",
    "    readout = True,\n",
    "    readout_format = 'd'\n",
    ")\n",
    "\n",
    "explanation_selector = wid.Dropdown(\n",
    "    options = [('95%',\"minimal\"), ('Full', \"full\")],\n",
    "    value = \"minimal\",\n",
    "    description = 'Explanation',\n",
    ")\n",
    "\n",
    "color_encoding_selector = wid.Dropdown(\n",
    "    options = f2t.feature_names + [\"prediction\", \"explanation_loss\"],\n",
    "    value = \"prediction\",\n",
    "    description = 'Color By',\n",
    ")\n",
    "\n",
    "ui = wid.HBox([cumulative_selector, explanation_selector, color_encoding_selector, num_datapoints_slider])\n",
    "\n",
    "output = wid.interactive_output(\n",
    "    f2t.visualize_datapoints,\n",
    "    {\n",
    "        \"cumulative\" : cumulative_selector,\n",
    "        \"num_datapoints\" : num_datapoints_slider,\n",
    "        \"explanation_type\" : explanation_selector,\n",
    "        \"color_encoding\" : color_encoding_selector\n",
    "    }\n",
    ")\n",
    "\n",
    "display(ui, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(pd.DataFrame(pd.DataFrame(np.abs(f2t.y-f2t.pred_y))).iloc[:4999,:])\\\n",
    ".transform_window(\n",
    "    sort=[{'field': 'cnt'}],\n",
    "    frame=[None, 0],\n",
    "    cumulative_count='count(*)',\n",
    ")\\\n",
    ".mark_rect().encode(\n",
    "    x = alt.X(\"cnt\", bin = alt.Bin(maxbins = 20)),\n",
    "    y = \"cumulative_count:Q\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_dataset = f2t.get_dataset(\"bike\")\n",
    "effect_additional_trees = []\n",
    "train_x, test_x, train_y, test_y\\\n",
    "= train_test_split(bike_dataset[\"x\"], bike_dataset[\"y\"], test_size = 0.3)\n",
    "for estimators in range(5, 300, 10):\n",
    "    for depth in range(1,5):\n",
    "        model = GradientBoostingRegressor(\n",
    "            n_estimators = estimators, \n",
    "            max_depth = depth, \n",
    "            learning_rate = 1\n",
    "        )\n",
    "        model.fit(train_x, train_y)\n",
    "        effect_additional_trees.append(\n",
    "            {\n",
    "                \"number of trees\" : estimators,\n",
    "                \"depth\" : depth, \n",
    "                \"train\" : r2_score(model.predict(train_x), train_y),\n",
    "                \"test\" : r2_score(model.predict(test_x), test_y)\n",
    "            }\n",
    "        )\n",
    "effect_additional_trees_df = pd.DataFrame(effect_additional_trees)\n",
    "effect_additional_trees_df = effect_additional_trees_df.melt(\n",
    "    id_vars = ['number of trees', \"depth\"],\n",
    "    var_name = 'set',\n",
    "    value_name = 'r_squared'\n",
    ")\n",
    "effect_additional_trees_df[\"outcome\"] = effect_additional_trees_df.apply(\n",
    "    lambda x: \"depth: \" + str(x[\"depth\"]) + \", set: \" + x[\"set\"],\n",
    "    axis = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(effect_additional_trees_df).mark_line().encode(\n",
    "    x = \"number of trees:Q\",\n",
    "    y = \"r_squared:Q\",\n",
    "    color = alt.Color(\"outcome\", type=\"nominal\", scale = alt.Scale(scheme = \"category20\"))\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
