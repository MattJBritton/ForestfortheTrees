{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RendererRegistry.enable('notebook')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#imports\n",
    "from __future__ import division\n",
    "import time\n",
    "import datetime\n",
    "import copy\n",
    "from itertools import product\n",
    "import operator\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import RandomForestClassifier,\n",
    "    GradientBoostingRegressor,\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "\n",
    "import altair as alt\n",
    "alt.renderers.enable(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForestForTheTrees:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.dataset = None\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        self.feature_names = None\n",
    "        self.feature_locs = None\n",
    "        self.target_type = None\n",
    "        self.classifier_type = None\n",
    "        self.classifier = None\n",
    "        self.feature_ranges = {}\n",
    "        self.mean_prediction = None\n",
    "        self.offset = None\n",
    "        \n",
    "    \n",
    "    def get_dataset(dataset):\n",
    "        \n",
    "        if dataset == \"breast cancer\":\n",
    "            dataLoad = datasets.load_breast_cancer(return_X_y=False)\n",
    "            return {\"x\": dataBunch.data[:,:10],\n",
    "                    \"y\": dataBunch.target,\n",
    "                    \"feature_names\": dataBunch.feature_names[:10],\n",
    "                    \"feature_locs\": {x:i for i,x in enumerate(dataBunch.feature_names[:10])},\n",
    "                    \"target_type\": \"Classification\"\n",
    "                   }     \n",
    "        elif dataset == \"cervical cancer\":\n",
    "            dataLoad = pd.read_csv(\"data/cervical_cancer.csv\")\n",
    "            target = dataLoad.Biopsy\n",
    "            dataLoad = dataLoad.drop([\"Person\", \"Biopsy\"],axis=1)\n",
    "            return {\"x\": dataLoad.values,\n",
    "                    \"y\": target,\n",
    "                    \"feature_names\": dataLoad.columns,\n",
    "                    \"feature_locs\": {x:i for i,x in enumerate(dataLoad.columns)},\n",
    "                    \"target_type\": \"Classification\"\n",
    "                   }\n",
    "        elif dataset == \"bike\":\n",
    "            def _datestr_to_timestamp(s):\n",
    "                return time.mktime(datetime.datetime.strptime(s, \"%Y-%m-%d\").timetuple())\n",
    "\n",
    "            dataLoad = pd.read_csv('data/bike.csv')\n",
    "            dataLoad['dteday'] = dataLoad['dteday'].apply(_datestr_to_timestamp)\n",
    "            dataLoad = pd.get_dummies(dataLoad, prefix=[\"weathersit\"], columns=[\"weathersit\"], drop_first=False)\n",
    "\n",
    "            #de-normalize data to produce human-readable features.\n",
    "            #Original range info from http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\n",
    "            dataLoad[\"hum\"] = dataLoad[\"hum\"].apply(lambda x: x*100.)\n",
    "            dataLoad[\"windspeed\"] = dataLoad[\"windspeed\"].apply(lambda x: x*67.)\n",
    "            #convert Celsius to Fahrenheit\n",
    "            dataLoad[\"temp\"] = dataLoad[\"temp\"].apply(lambda x: (x*47. - 8)*9/5 +32)\n",
    "            dataLoad[\"atemp\"] = dataLoad[\"atemp\"].apply(lambda x: (x*66. - 16)*9/5 + 32)\n",
    "\n",
    "            #rename features to make them interpretable for novice users\n",
    "            feature_names_dict = {\n",
    "                \"yr\":\"First or Second Year\", \n",
    "                \"season\":\"Season\", \n",
    "                \"hr\":\"Hour of Day\", \n",
    "                \"workingday\":\"Work Day\",\n",
    "                \"weathersit_2\":\"Misty Weather\",\n",
    "                \"weathersit_3\":\"Light Precipitation\",\n",
    "                \"weathersit_4\":\"Heavy Precipitation\",\n",
    "                \"temp\":\"Temperature (F)\",\n",
    "                \"atemp\":\"Feels Like (F)\",\n",
    "                \"hum\":\"Humidity\",\n",
    "                \"windspeed\":\"Wind Speed\"\n",
    "            }\n",
    "            dataLoad = dataLoad.rename(mapper=feature_names_dict,axis=1) \n",
    "            features = feature_names_dict.values()\n",
    "\n",
    "            return {\"x\": dataLoad[features].values,\n",
    "                    \"y\": dataLoad[\"cnt\"],\n",
    "                    \"feature_names\": features,\n",
    "                    \"feature_locs\": {x:i for i,x in enumerate(features)},\n",
    "                    \"target_type\": \"Regression\"\n",
    "                   }\n",
    "        \n",
    "    def load_dataset(self, dataset):\n",
    "\n",
    "        self.dataset = dataset\n",
    "        data = get_dataset(self.dataset)\n",
    "        self.x = data.x\n",
    "        self.y = data.y\n",
    "        self.feature_names = data.feature_names\n",
    "        self.feature_locs = data.feature_locs\n",
    "        self.target_type = data.target_type\n",
    "        \n",
    "        #get feature quantiles\n",
    "        for feature in self.feature_names:\n",
    "            self.feature_ranges[feature] = self.get_quantiles(feature)        \n",
    "            \n",
    "    def build_base_model(self, num_estimators, model_type, learning_rate):\n",
    "\n",
    "        model_lookup_dict = {\n",
    "            (\"classification\", \"random forest\") : RandomForestClassifier,\n",
    "            (\"classification\", \"gradient boosting\") : GradientBoostingClassifier,\n",
    "            (\"regression\", \"random forest\") : RandomForestRegressor,\n",
    "            (\"regression\", \"gradient boosting\") : GradientBoostingRegressor\n",
    "        }\n",
    "\n",
    "        self.model_type = model_type\n",
    "        self.classifier_type = model_lookup_dict[(self.target_type, self.model_type)]\n",
    "\n",
    "        self.model = self.classifier_type(n_estimators=num_estimators, max_depth=2)\n",
    "        self.model.fit(self.x, self.y)\n",
    "        self.pred_y = model.predict(self.x)\n",
    "\n",
    "    def get_model_accuracy(self):\n",
    "        if self.target_type == \"classification\":\n",
    "            return accuracy_score(self.y, self.pred_y)\\\n",
    "        else:\n",
    "            return r2_score(self.y, self.pred_y)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(self, num_tiles=20, collapse_1d = True, quantiles=False):\n",
    "    \n",
    "    def _get_coordinate_matrix(lst, length, direction):\n",
    "        if direction==\"h\":\n",
    "            return lst*length\n",
    "        else:\n",
    "            return [item for item in lst\\\n",
    "             for i in range(length)]   \n",
    "\n",
    "    def get_quantile_matrix(self, feat1, feat2):\n",
    "        h = _get_coordinate_matrix(\n",
    "            list(self.feature_ranges[feat1]),\n",
    "            len(self.feature_ranges[feat2]),\n",
    "            \"h\"\n",
    "        )\n",
    "        v = _get_coordinate_matrix(\n",
    "            list(self.feature_ranges[feat2]),\n",
    "            len(self.feature_ranges[feat1]),\n",
    "            \"v\"\n",
    "        )                      \n",
    "        return h,v \n",
    "\n",
    "    def get_leaf_value(self, model, node_position):\n",
    "        if self.target_type == \"Classification\":\n",
    "            return self.model.tree_.value[node_position][0][1]/\\\n",
    "                        (model.tree_.value[node_position][0][1] + model.tree_.value[node_position][0][0])\n",
    "        else:\n",
    "            return model.tree_.value[node_position]\n",
    "        \n",
    "    def get_feature_pair_key(self, feat1, feat2):\n",
    "        if self.feature_ranges[feat1].shape[0] == self.feature_ranges[feat2].shape[0]:\n",
    "            #need stable order so keys with same number of quantiles appear in only one order\n",
    "            return tuple(sorted([feat1, feat2]))\n",
    "        elif self.feature_ranges[feat1].shape[0] > self.feature_ranges[feat2].shape[0]:\n",
    "            return tuple([feat1, feat2])\n",
    "        else:\n",
    "            return tuple([feat2, feat1])\n",
    "        \n",
    "    def get_quantiles(self, feat):\n",
    "        loc = self.feature_locs[feat]\n",
    "        if np.unique(self.x[:,loc]).shape[0] < 30 or type(self.x[0,loc]) is str: #is categorical/ordinal?\n",
    "            return np.unique(self.x[:,loc])\n",
    "        else:\n",
    "            if quantiles:\n",
    "                return np.around(\n",
    "                    np.unique(\n",
    "                        np.quantile(\n",
    "                            a=self.x[:,loc],\n",
    "                            q=np.linspace(0, 1, num_tiles)\n",
    "                        )\n",
    "                    ),\n",
    "                    1)\n",
    "            else:\n",
    "                return np.around(\n",
    "                    np.linspace(\n",
    "                        np.min(self.x[:,loc]), \n",
    "                        np.max(self.x[:,loc]),\n",
    "                        num_tiles\n",
    "                    )\n",
    "                    ,1)\n",
    "\n",
    "    def reduce_to_1d(self, arr, threshold, direction):\n",
    "        if direction == \"h\":\n",
    "            reduced_arr = arr - arr[:,0].reshape(-1,1)\n",
    "        else:\n",
    "            reduced_arr = arr - arr[0,:].reshape(1,-1)\n",
    "        bol_reduce = (np.max(np.abs(reduced_arr))/np.max(np.abs(arr))) <= threshold\n",
    "        return bol_reduce     \n",
    "        \n",
    "    #generate data structure for pairwise charts\n",
    "    feature_pairs = {self.get_feature_pair_key(key[0], key[1]) : \n",
    "                     {\n",
    "                         \"map\":None,\n",
    "                         \"predicates\":[]\n",
    "                     }\n",
    "                     for key in [tuple(t) for t in product(self.feature_names, repeat=2)]}      \n",
    "  \n",
    "    for key, value in feature_pairs.iteritems():\n",
    "        h, v = self.get_quantile_matrix(key[0], key[1])\n",
    "        value[\"map\"] = np.array(\n",
    "            [\n",
    "                {\n",
    "                    key[0]:x,\n",
    "                    key[1]:y\n",
    "                }\n",
    "                for x,y in zip(h,v)]).reshape(len(self.feature_ranges[key[1]]), len(self.feature_ranges[key[0]]))\n",
    "        \n",
    "    for modelT in self.model.estimators_:\n",
    "        if self.target_type == \"regression\":\n",
    "            model = modelT[0]\n",
    "        else:\n",
    "            model = modelT\n",
    "        feature_ids = {i : {\n",
    "            \"number\":x,\n",
    "            \"name\":self.feature_names[x]\n",
    "        } for i,x in enumerate(list(self.model.tree_.feature)) if x>=0} #-2 means leaf node\n",
    "\n",
    "        #for 1-layer trees\n",
    "        if self.model.tree_.feature[1] <0:\n",
    "            print \"1-layer tree\"\n",
    "            print key\n",
    "            feature_pair_key = self.get_feature_pair_key(\n",
    "                feature_ids[0][\"name\"],\n",
    "                feature_ids[0][\"name\"]\n",
    "            )\n",
    "            decision_func_dict = {\n",
    "                \"feature_name\": feature_ids[0][\"name\"],\n",
    "                \"threshold\": model.tree_.threshold[0],\n",
    "                \"operator\": operator.le,\n",
    "                \"prob_le\": get_leaf_value(model,1),\n",
    "                \"prob_gt\": get_leaf_value(model,2)\n",
    "            }       \n",
    "            #build the predictive function used in the decision tree\n",
    "            def dt_predicate(data_case, decision_func_dict=decision_func_dict):\n",
    "                if decision_func_dict[\"operator\"](\\\n",
    "                                                    data_case[decision_func_dict[\"feature_name\"]],\\\n",
    "                                                    decision_func_dict[\"threshold\"]\\\n",
    "                                                   ):\n",
    "                    return decision_func_dict[\"prob_le\"]\n",
    "                else:\n",
    "                    return decision_func_dict[\"prob_gt\"]        \n",
    "        else:\n",
    "            for node_position in [1,4]: #positions for left and right nodes at layer 2\n",
    "                if node_position in feature_ids:\n",
    "                    feature_pair_key = get_feature_pair_key(feature_ids[0][\"name\"], feature_ids[node_position][\"name\"])\n",
    "                    #get the decision rules\n",
    "                    decision_func_dict = {\n",
    "                        \"feature_name_1\": feature_ids[0][\"name\"],\n",
    "                        \"threshold_1\": model.tree_.threshold[0],\n",
    "\n",
    "                        \"operator_1\": operator.le if node_position == 1 else operator.gt,\n",
    "\n",
    "                        \"feature_name_2\": feature_ids[node_position][\"name\"],\n",
    "                        \"threshold_2\": model.tree_.threshold[node_position],\n",
    "\n",
    "                        \"operator_2\": operator.le,\n",
    "\n",
    "                        \"prob_le\": get_leaf_value(model,node_position+1),\n",
    "\n",
    "                        \"prob_gt\": get_leaf_value(model,node_position+2)\n",
    "                    }\n",
    "                    #print decision_func_dict\n",
    "                    #build the predictive function used in the decision tree\n",
    "                    def dt_predicate(data_case, decision_func_dict=decision_func_dict):\n",
    "                        #print data_case\n",
    "                        #print decision_func_dict\n",
    "                        if decision_func_dict[\"operator_1\"](\\\n",
    "                                                            data_case[decision_func_dict[\"feature_name_1\"]],\\\n",
    "                                                            decision_func_dict[\"threshold_1\"]\\\n",
    "                                                           ):\n",
    "                            #print \"in1\"\n",
    "                            if decision_func_dict[\"operator_2\"](\\\n",
    "                                                                data_case[decision_func_dict[\"feature_name_2\"]],\\\n",
    "                                                                decision_func_dict[\"threshold_2\"]\\\n",
    "                                                               ):\n",
    "                                #print \"in2\"\n",
    "                                return decision_func_dict[\"prob_le\"]\n",
    "                            else:\n",
    "                                #print \"in3\"\n",
    "                                return decision_func_dict[\"prob_gt\"]\n",
    "                        else:\n",
    "                            return 0.\n",
    "\n",
    "                else: #asymmetric tree, this is a leaf node\n",
    "                    feature_pair_key = get_feature_pair_key(feature_ids[0][\"name\"], feature_ids[0][\"name\"])\n",
    "                    decision_func_dict = {\n",
    "                        \"feature_name\": feature_ids[0][\"name\"],\n",
    "                        \"threshold\": model.tree_.threshold[0],\n",
    "                        \"operator\": operator.le if node_position == 1 else operator.gt,\n",
    "                        \"prob\": model.tree_.value[node_position]\n",
    "                    }\n",
    "                    #build the predictive function used in the decision tree\n",
    "                    def dt_predicate(data_case, decision_func_dict=decision_func_dict):\n",
    "                        #print data_case\n",
    "                        if decision_func_dict[\"operator\"](\\\n",
    "                                                            data_case[decision_func_dict[\"feature_name\"]],\\\n",
    "                                                            decision_func_dict[\"threshold\"]\\\n",
    "                                                           ):\n",
    "                            return decision_func_dict[\"prob\"]\n",
    "                        else:                         \n",
    "                            return 0.                 \n",
    "\n",
    "                feature_pairs[feature_pair_key][\"predicates\"].append(dt_predicate)\n",
    "            \n",
    "    #now calculate output array for each feature pair\n",
    "    for key, value in feature_pairs.iteritems():\n",
    "        arrs = []\n",
    "        for predicate in value[\"predicates\"]:\n",
    "            f = np.vectorize(predicate)\n",
    "            arrs.append(f(value[\"map\"]))\n",
    "        if len(arrs) > 0:\n",
    "            #details of vote aggreggation method for random forest\n",
    "            #https://stats.stackexchange.com/questions/127077/random-forest-probabilistic-prediction-vs-majority-vote\n",
    "            value[\"output\"] = np.sum(np.stack(arrs, axis=-1), axis=-1)\n",
    "        else:\n",
    "            value[\"output\"] = None\n",
    "            \n",
    "    #build chart data\n",
    "    for key, value in feature_pairs.iteritems():\n",
    "        h,v = get_quantile_matrix(key[0], key[1])\n",
    "        value[\"H_Indices\"] = h\n",
    "        value[\"V_Indices\"] = v\n",
    "        #value[\"Votes\"] = value[\"output\"].ravel() if value[\"output\"] is not None else None       \n",
    "        \n",
    "    output_details = {\"offset\":0.,\n",
    "                      \"no_predictor_features\":[],\n",
    "                      \"1d_features\":[],\n",
    "                      \"dropped_features\":[]}\n",
    "    chart_data = {}\n",
    "    for key, value in feature_pairs.iteritems(): \n",
    "        newKey = key\n",
    "        if value[\"output\"] is None:\n",
    "            output_details[\"no_predictor_features\"].append(key)\n",
    "            value[\"removed\"] = True\n",
    "        else:          \n",
    "            if collapse_1d:\n",
    "                if reduce_to_1d(value[\"output\"], 0., \"v\"):\n",
    "                    newKey = key[1]\n",
    "                    value[\"output\"] = value[\"output\"][0,:]\n",
    "                    value[\"H_Indices\"] = feature_ranges[newKey]\n",
    "                    value[\"V_Indices\"] = None\n",
    "                    value[\"1d_key\"] = newKey\n",
    "                    value[\"removed\"] = True\n",
    "                    output_details[\"1d_features\"].append(key)                 \n",
    "                elif reduce_to_1d(value[\"output\"], 0., \"h\"):\n",
    "                    newKey = key[0]\n",
    "                    value[\"output\"] = value[\"output\"][:,0]\n",
    "                    value[\"H_Indices\"] = feature_ranges[newKey]\n",
    "                    value[\"V_Indices\"] = None\n",
    "                    value[\"1d_key\"] = newKey\n",
    "                    value[\"removed\"] = True\n",
    "                    output_details[\"1d_features\"].append(key)\n",
    "\n",
    "            #subtract mean from votes to center at zero\n",
    "            vote_mean = value[\"output\"].mean()\n",
    "            output_details[\"offset\"] += vote_mean\n",
    "            value[\"output\"] = value[\"output\"] - vote_mean\n",
    "\n",
    "                    \n",
    "    #do another loop through chart_data to push 1d charts into 2d\n",
    "    if collapse_1d:\n",
    "        for value in feature_pairs.itervalues():\n",
    "            if value[\"V_Indices\"] is None:\n",
    "                key = value[\"1d_key\"]\n",
    "                #print key, value[\"output\"]\n",
    "                #get list of charts with this feature\n",
    "                matchList = sorted([{\"key\": kInner, \"feature_importance\": np.std(vInner[\"output\"])}\\\n",
    "                                    for kInner, vInner in feature_pairs.iteritems()\\\n",
    "                                    if \"removed\" not in vInner and key in kInner],\\\n",
    "                                   key=lambda x: x[\"feature_importance\"], reverse=True)\n",
    "                #print matchList\n",
    "                if len(matchList) > 0:\n",
    "                    #value[\"removed\"] = True\n",
    "                    matchKey = matchList[0][\"key\"]\n",
    "                    #match_length = feature_pairs[matchKey][\"output\"].shape[0]\n",
    "                    #print match_length\n",
    "                    #print feature_pairs[matchKey][\"output\"]\n",
    "                    feature_pairs[matchKey][\"output\"] = feature_pairs[matchKey][\"output\"]\\\n",
    "                    + value[\"output\"].reshape(\\\n",
    "                                              -1 if key==matchKey[1] else 1,\\\n",
    "                                              -1 if key==matchKey[0] else 1\\\n",
    "                                             )\n",
    "    \n",
    "    #one last loop to generate the horizontal and vertical components\n",
    "    for key, value in feature_pairs.iteritems():\n",
    "        if \"removed\" in value:\n",
    "            pass\n",
    "        else:\n",
    "            value[\"output_H\"] = np.mean(value[\"output\"], axis=1).reshape(-1,1)\n",
    "            value[\"output_V\"] = np.mean(value[\"output\"], axis=0).reshape(1,-1)\n",
    "            value[\"output_HReduced\"] = np.mean(value[\"output\"] - value[\"output_V\"].reshape(1,-1), axis=1)\\\n",
    "            .reshape(1,-1)\n",
    "            value[\"output_VReduced\"] = np.mean(value[\"output\"] - value[\"output_H\"].reshape(-1,1), axis=0)\\\n",
    "            .reshape(-1,1)\n",
    "\n",
    "    #remove deleted keys\n",
    "    feature_pairs = {key:val for key, val in feature_pairs.iteritems() if \"removed\" not in val}\n",
    "    feauture_pairs = OrderedDict(sorted(feature_pairs.items(),\\\n",
    "                                        key=lambda x: np.std(x[1][\"output\"]), reverse=True))\n",
    "    return feature_pairs, feature_ranges, output_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_chart(data, raw_data, scheme=\"redyellowblue\", plot_points=True, fields_subset=None):\n",
    "    i = 1\n",
    "    rows = []\n",
    "    charts = []\n",
    "    for key, value in data.iteritems():\n",
    "        \n",
    "        if fields_subset is None or key in fields_subset:\n",
    "            \n",
    "            if value[\"dimension\"] == 2:\n",
    "                y_encoding = alt.Y(field=\"V_Indices\",\n",
    "                                   type=\"ordinal\", sort=\"descending\",\n",
    "                                   axis=alt.Axis(title=key[1]))                \n",
    "\n",
    "            x_encoding = alt.X(field=\"H_Indices\",\n",
    "                               type=\"ordinal\", sort=\"ascending\",\n",
    "                               axis=alt.Axis(title=key[0] if value[\"dimension\"] == 2 else key))\n",
    "\n",
    "            color_encoding = alt.Color(field= \"Votes\",\n",
    "                                       type=\"quantitative\",\n",
    "                                       scale=alt.Scale(scheme=scheme),\n",
    "                                       legend=alt.Legend(title=\"Votes\"))\n",
    "            chart = alt.Chart(data=value[\"df\"]).mark_rect()\n",
    "            if value[\"dimension\"] == 1:\n",
    "                chart = chart.encode(x=x_encoding, color=color_encoding)\\\n",
    "                    .properties(width=150, height=20)\n",
    "            else:\n",
    "                 chart = chart.encode(x=x_encoding, y=y_encoding, color=color_encoding)\\\n",
    "                    .properties(width=150, height=150)\n",
    "\n",
    "            if plot_points:\n",
    "                df = pd.DataFrame(raw_data[\"data\"][np.random.choice(raw_data[\"data\"].shape[0],300,replace=False),:],\\\n",
    "                                  columns=raw_data[\"feature_names\"])\n",
    "                points = alt.Chart(df).mark_circle(\n",
    "                    color='black',\n",
    "                    size=5\n",
    "                ).encode(\n",
    "                    x=alt.X(field=key[0],type=\"quantitative\", sort=\"ascending\", axis=None),\n",
    "                    y=alt.X(field=key[1],type=\"quantitative\", sort=\"ascending\", axis=None)\n",
    "                ).properties(width=150, height=150)\n",
    "                chart = chart + points\n",
    "\n",
    "            charts.append(chart)\n",
    "        if len(charts)==4 or i==len(data):\n",
    "            rows.append(alt.hconcat(*charts))\n",
    "            charts = []\n",
    "        i += 1\n",
    "    return alt.vconcat(*rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data, \n",
    "             raw_data, \n",
    "             feature_ranges, \n",
    "             classifier, \n",
    "             chart_details, \n",
    "             sample_size,\n",
    "             fidelity_threshold=1., \n",
    "             rollup=None\n",
    "            ):\n",
    "    sample_offset = 100\n",
    "    def get_predictions_base():\n",
    "        return np.full((sample_size,1),chart_details[\"offset\"] + np.mean(raw_data[\"target\"]))\n",
    "    \n",
    "    def get_explanation_accuracy(explanation_predictions):\n",
    "        if raw_data[\"target_type\"] == \"Regression\":\n",
    "            return r2_score(model_predictions, explanation_predictions)\n",
    "        \n",
    "    def get_prediction_contributions(chart, data_positions):\n",
    "        return np.take(chart, data_positions)   \n",
    "    \n",
    "    def sum_arrays(arrMain, arrAdd, horizontal, keyMain, keyAdd):\n",
    "        #print arrMain.shape\n",
    "        #print arrAdd.shape\n",
    "        return arrMain + arrAdd.reshape(arrMain.shape[0] if\\\n",
    "                                        (keyMain[1]==keyAdd[1] or keyMain[1]==keyAdd[0]) else 1, -1)\n",
    "    \n",
    "    def bin_data():\n",
    "    \n",
    "        prediction_contributions = pd.DataFrame(\n",
    "            raw_data[\"data\"][sample_offset:sample_offset+sample_size,:],\n",
    "            columns=raw_data[\"feature_names\"]\n",
    "        )\n",
    "        for key, value in data.iteritems():\n",
    "            prediction_contributions[\"tempH\"] = np.digitize(prediction_contributions.loc[:,key[0]],\n",
    "                                                            feature_ranges[key[0]])-1.\n",
    "            prediction_contributions[\"tempV\"] = np.digitize(prediction_contributions.loc[:,key[1]],\n",
    "                                                            feature_ranges[key[1]])-1.\n",
    "            prediction_contributions[key] = prediction_contributions.apply(lambda x:\\\n",
    "                                                    int(x[\"tempV\"]*len(feature_ranges[key[0]]) + x[\"tempH\"]),\\\n",
    "                                                    axis=1)\n",
    "        return prediction_contributions.loc[:,[key for key in data.iterkeys()]]\n",
    "\n",
    "    def evaluate_single_explanation(explanation):\n",
    "        prediction_contributions = bin_data()\n",
    "        model_predictions = classifier.predict(raw_data[\"data\"][sample_offset:sample_offset+sample_size,:])\n",
    "        \n",
    "        chart_values = {key: {\n",
    "            \"output\" : val[\"output\"],\n",
    "            \"output_VReduced\" : val[\"output_VReduced\"],\n",
    "            \"output_H\" : val[\"output_H\"],\n",
    "            \"output_HReduced\" : val[\"output_HReduced\"],\n",
    "            \"output_V\" : val[\"output_V\"]\n",
    "        } for key, val in data.iteritems()}\n",
    "        \n",
    "        get_explanation_accuracy(\n",
    "            get_predictions_base() +\\\n",
    "            np.sum(np.array([\n",
    "                get_prediction_contributions(\n",
    "                    temp_outputs[expKey][\"output\"],\n",
    "                    prediction_contributions[expKey])\n",
    "                for expKey in keysInCurrentExplanation]),\n",
    "                   axis=0).reshape(-1,1)\n",
    "        )\n",
    "        \n",
    "    explanation = []\n",
    "    evaluation_details = {}        \n",
    "    evaluation_details[0] = {\"score\": get_explanation_accuracy(base_predictions)}    \n",
    "    \n",
    "    i = 1\n",
    "    while fidelity < fidelity_threshold and len(explanation) < len(data):\n",
    "        evaluation_details[i] = {}\n",
    "        keys_to_evaluate = [key for key in data.iterkeys() if key not in explanation]\n",
    "        for key in keys_to_evaluate:\n",
    "            #print \"key to evaluate: \" + str(key)\n",
    "            #roll up other keys\n",
    "            keysInCurrentExplanation = explanation+[key]\n",
    "            temp_outputs = copy.deepcopy(chart_values)\n",
    "            if rollup == \"advanced\":\n",
    "                for keyRollup in [k for k in keys_to_evaluate if k != key]:\n",
    "                    #print \"rollup: \" + str(keyRollup)\n",
    "                    hUsed = False\n",
    "                    vUsed = False\n",
    "                    for keyExisting in keysInCurrentExplanation:\n",
    "                        #print \"try against: \" + str(keyExisting)\n",
    "                        if (keyRollup[1] == keyExisting[0] or keyRollup[1] == keyExisting[1]) and not hUsed:\n",
    "                            hUsed = True\n",
    "                            if vUsed:\n",
    "                                #print \"HReduce\"\n",
    "                                temp_outputs[keyExisting][\"output\"] = sum_arrays(\n",
    "                                    temp_outputs[keyExisting][\"output\"],\n",
    "                                    temp_outputs[keyRollup][\"output_HReduced\"],\n",
    "                                    True, \n",
    "                                    keyExisting, \n",
    "                                    keyRollup\n",
    "                                )\n",
    "                                break\n",
    "                            else:\n",
    "                                #print \"HAll\"\n",
    "                                temp_outputs[keyExisting][\"output\"] = sum_arrays(\n",
    "                                    temp_outputs[keyExisting][\"output\"],\n",
    "                                    temp_outputs[keyRollup][\"output_H\"],\n",
    "                                    True, \n",
    "                                    keyExisting, \n",
    "                                    keyRollup\n",
    "                                )                           \n",
    "                        elif (keyRollup[0] == keyExisting[0] or keyRollup[0] == keyExisting[1]) and not vUsed:\n",
    "                            vUsed = True\n",
    "                            if hUsed:\n",
    "                                #print \"VReduce\"\n",
    "                                temp_outputs[keyExisting][\"output\"] = sum_arrays(\n",
    "                                    temp_outputs[keyExisting][\"output\"],\n",
    "                                    temp_outputs[keyRollup][\"output_VReduced\"],\n",
    "                                    False, \n",
    "                                    keyExisting, \n",
    "                                    keyRollup\n",
    "                                )                          \n",
    "                                break\n",
    "                            else:\n",
    "                                #print \"VAll\"\n",
    "                                temp_outputs[keyExisting][\"output\"] = sum_arrays(\n",
    "                                    temp_outputs[keyExisting][\"output\"],\n",
    "                                    temp_outputs[keyRollup][\"output_V\"],\n",
    "                                    False, \n",
    "                                    keyExisting, \n",
    "                                    keyRollup\n",
    "                                )\n",
    "    \n",
    "            evaluation_details[i][key] =\\\n",
    "            get_explanation_accuracy(base_predictions +\\\n",
    "                                     np.sum(np.array([\\\n",
    "                                                      get_prediction_contributions(temp_outputs[expKey][\"output\"],\n",
    "                                                                                  prediction_contributions[expKey])\\\n",
    "                                                     for expKey in keysInCurrentExplanation]),\\\n",
    "                                            axis=0).reshape(-1,1)\\\n",
    "                                    )\n",
    "            \n",
    "        #get key with highest fidelity score\n",
    "        best_key = max(evaluation_details[i].iterkeys(),\\\n",
    "            key=(lambda key: evaluation_details[i][key]))\n",
    "        explanation.append(best_key)\n",
    "        evaluation_details[i][\"best_key\"] = best_key\n",
    "        \n",
    "        if rollup == \"simple\":\n",
    "            #roll up other keys\n",
    "            temp_outputs = copy.deepcopy(chart_values)\n",
    "            for keyRollup in [k for k in data.keys() if k not in explanation]:\n",
    "                #print \"rollup: \" + str(keyRollup)\n",
    "                hUsed = False\n",
    "                vUsed = False\n",
    "                for keyExisting in explanation:\n",
    "                    #print \"try against: \" + str(keyExisting)\n",
    "                    if (keyRollup[1] == keyExisting[0] or keyRollup[1] == keyExisting[1]) and not hUsed:\n",
    "                        hUsed = True\n",
    "                        if vUsed:\n",
    "                            #print \"HReduce\"\n",
    "                            temp_outputs[keyExisting][\"output\"] = sum_arrays(\n",
    "                                temp_outputs[keyExisting][\"output\"],\n",
    "                                temp_outputs[keyRollup][\"output_HReduced\"],\n",
    "                                True, \n",
    "                                keyExisting, \n",
    "                                keyRollup\n",
    "                            )\n",
    "                            break\n",
    "                        else:\n",
    "                            #print \"HAll\"\n",
    "                            temp_outputs[keyExisting][\"output\"] = sum_arrays(\n",
    "                                temp_outputs[keyExisting][\"output\"],\n",
    "                                temp_outputs[keyRollup][\"output_H\"],\n",
    "                                True, \n",
    "                                keyExisting, \n",
    "                                keyRollup\n",
    "                            )                           \n",
    "                    elif (keyRollup[0] == keyExisting[0] or keyRollup[0] == keyExisting[1]) and not vUsed:\n",
    "                        vUsed = True\n",
    "                        if hUsed:\n",
    "                            #print \"VReduce\"\n",
    "                            temp_outputs[keyExisting][\"output\"] = sum_arrays(\n",
    "                                temp_outputs[keyExisting][\"output\"],\n",
    "                                temp_outputs[keyRollup][\"output_VReduced\"],\n",
    "                                False, \n",
    "                                keyExisting, \n",
    "                                keyRollup\n",
    "                            )                          \n",
    "                            break\n",
    "                        else:\n",
    "                            #print \"VAll\"\n",
    "                            temp_outputs[keyExisting][\"output\"] = sum_arrays(\n",
    "                                temp_outputs[keyExisting][\"output\"],\n",
    "                                temp_outputs[keyRollup][\"output_V\"],\n",
    "                                False, \n",
    "                                keyExisting, \n",
    "                                keyRollup\n",
    "                            )        \n",
    "\n",
    "            evaluation_details[i][\"score\"] =\\\n",
    "            get_explanation_accuracy(base_predictions +\\\n",
    "                                     np.sum(np.array([\\\n",
    "                                                      get_prediction_contributions(temp_outputs[expKey][\"output\"],\n",
    "                                                                                  prediction_contributions[expKey])\\\n",
    "                                                     for expKey in explanation]),\\\n",
    "                                            axis=0).reshape(-1,1)\\\n",
    "                                    )\n",
    "        else:\n",
    "            evaluation_details[i][\"score\"] = evaluation_details[i][best_key]\n",
    "        \n",
    "        fidelity = evaluation_details[i][\"score\"]\n",
    "        print  \"*******\"\n",
    "        print i\n",
    "        print best_key\n",
    "        print evaluation_details[i][\"score\"]\n",
    "        i += 1\n",
    "            \n",
    "    \n",
    "    return explanation, evaluation_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******\n",
      "1\n",
      "('Temperature (F)', 'Hour of Day')\n",
      "0.7285758589470361\n",
      "*******\n",
      "2\n",
      "('Hour of Day', 'Work Day')\n",
      "0.8732662910184066\n",
      "*******\n",
      "3\n",
      "('Feels Like (F)', 'Hour of Day')\n",
      "0.9098027166722784\n",
      "*******\n",
      "4\n",
      "('Hour of Day', 'Season')\n",
      "0.9288196703317801\n",
      "*******\n",
      "5\n",
      "('Feels Like (F)', 'Light Precipitation')\n",
      "0.93330500421946\n",
      "*******\n",
      "6\n",
      "('Feels Like (F)', 'Temperature (F)')\n",
      "0.9386753598101099\n",
      "*******\n",
      "7\n",
      "('Feels Like (F)', 'Work Day')\n",
      "0.9415854467993299\n",
      "*******\n",
      "8\n",
      "('Temperature (F)', 'Work Day')\n",
      "0.9425377247731114\n",
      "*******\n",
      "9\n",
      "('Season', 'Light Precipitation')\n",
      "0.9440507978997703\n",
      "*******\n",
      "10\n",
      "('Temperature (F)', 'Misty Weather')\n",
      "0.9443240168286298\n",
      "*******\n",
      "11\n",
      "('Feels Like (F)', 'Misty Weather')\n",
      "0.9445870087732237\n",
      "*******\n",
      "12\n",
      "('Feels Like (F)', 'Season')\n",
      "0.9446712018066583\n",
      "*******\n",
      "13\n",
      "('Season', 'Misty Weather')\n",
      "0.9445262729411412\n",
      "*******\n",
      "14\n",
      "('Hour of Day', 'Misty Weather')\n",
      "0.9438960290581635\n",
      "*******\n",
      "15\n",
      "('Hour of Day', 'Light Precipitation')\n",
      "0.9278034987690513\n",
      "*******\n",
      "16\n",
      "('Hour of Day', 'First or Second Year')\n",
      "0.921547161305814\n",
      "*******\n",
      "17\n",
      "('Wind Speed', 'Hour of Day')\n",
      "0.9632899189077831\n"
     ]
    }
   ],
   "source": [
    "data_to_use = load_data(\"bike\")\n",
    "classifier, accuracy = build_classifier(data_to_use, 100, \"gradient boosting\", 1.0)\n",
    "chart_data, feature_ranges, output_details = calculate(data_to_use, classifier, 100, True)\n",
    "#generate_chart(chart_data, data_to_use)\n",
    "explanation, _ = evaluate(chart_data, data_to_use, feature_ranges, classifier, output_details, 5000, .95, \"advanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******\n",
      "1\n",
      "('Temperature (F)', 'Hour of Day')\n",
      "0.6388493180214152\n",
      "*******\n",
      "2\n",
      "('Hour of Day', 'Work Day')\n",
      "0.7470833050834247\n",
      "*******\n",
      "3\n",
      "('Feels Like (F)', 'Hour of Day')\n",
      "0.8359935497942745\n",
      "*******\n",
      "4\n",
      "('Hour of Day', 'First or Second Year')\n",
      "0.8756325672500528\n",
      "*******\n",
      "5\n",
      "('Hour of Day', 'Light Precipitation')\n",
      "0.9390950406634805\n",
      "*******\n",
      "6\n",
      "('Humidity', 'Hour of Day')\n",
      "0.9430382677976129\n",
      "*******\n",
      "7\n",
      "('Hour of Day', 'Season')\n",
      "0.9475764882968291\n",
      "*******\n",
      "8\n",
      "('Feels Like (F)', 'Wind Speed')\n",
      "0.955816726136186\n"
     ]
    }
   ],
   "source": [
    "data_to_use = load_data(\"bike\")\n",
    "classifier, accuracy = build_classifier(data_to_use, 100, \"gradient boosting\", 1.0)\n",
    "chart_data, feature_ranges, output_details = calculate(data_to_use, classifier, 100, True)\n",
    "#generate_chart(chart_data, data_to_use)\n",
    "explanation, _ = evaluate(chart_data, data_to_use, feature_ranges, classifier, output_details, 5000, .95, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******\n",
      "1\n",
      "('Temperature (F)', 'Hour of Day')\n",
      "0.7285758589470364\n",
      "*******\n",
      "2\n",
      "('Hour of Day', 'Work Day')\n",
      "0.873266291018407\n",
      "*******\n",
      "3\n",
      "('Feels Like (F)', 'Hour of Day')\n",
      "0.9098027166722783\n",
      "*******\n",
      "4\n",
      "('Hour of Day', 'First or Second Year')\n",
      "0.8552949201373392\n",
      "*******\n",
      "5\n",
      "('Hour of Day', 'Light Precipitation')\n",
      "0.9216284155559572\n",
      "*******\n",
      "6\n",
      "('Humidity', 'Hour of Day')\n",
      "0.9314859629111633\n",
      "*******\n",
      "7\n",
      "('Hour of Day', 'Season')\n",
      "0.9368087989378879\n",
      "*******\n",
      "8\n",
      "('Feels Like (F)', 'Wind Speed')\n",
      "0.9432563360125976\n",
      "*******\n",
      "9\n",
      "('Humidity', 'Light Precipitation')\n",
      "0.9462162399169958\n",
      "*******\n",
      "10\n",
      "('Feels Like (F)', 'Work Day')\n",
      "0.9497786473664296\n",
      "*******\n",
      "11\n",
      "('Feels Like (F)', 'Misty Weather')\n",
      "0.949079932793873\n",
      "*******\n",
      "12\n",
      "('Humidity', 'Season')\n",
      "0.9510021779014951\n"
     ]
    }
   ],
   "source": [
    "#data_to_use = load_data(\"bike\")\n",
    "#classifier, accuracy = build_classifier(data_to_use, 100, \"gradient boosting\", 1.0)\n",
    "#chart_data, feature_ranges, output_details = calculate(data_to_use, classifier, 100, True)\n",
    "#generate_chart(chart_data, data_to_use)\n",
    "explanation, _ = evaluate(chart_data, data_to_use, feature_ranges, classifier, output_details, 5000, .95, \"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_chart(chart_data, data_to_use, fields_subset = explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_chart(chart_data, data_to_use)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
